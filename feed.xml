<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en, bg, de"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://www.kparvanov.com/feed.xml" rel="self" type="application/atom+xml"/><link href="https://www.kparvanov.com/" rel="alternate" type="text/html" hreflang="en, bg, de"/><updated>2025-06-30T01:32:18+00:00</updated><id>https://www.kparvanov.com/feed.xml</id><title type="html">blank</title><subtitle>My name is Kaloyan Parvanov and I am a MS in Applied Mathematics graduate from the University of Colorado Boulder. This is my personal website where I plan to share my projects, publications, and blog posts. </subtitle><entry><title type="html">The St. Petersburg Paradox, Re-Run Over Time</title><link href="https://www.kparvanov.com/blog/2025/st_petersburg/" rel="alternate" type="text/html" title="The St. Petersburg Paradox, Re-Run Over Time"/><published>2025-06-29T01:19:00+00:00</published><updated>2025-06-29T01:19:00+00:00</updated><id>https://www.kparvanov.com/blog/2025/st_petersburg</id><content type="html" xml:base="https://www.kparvanov.com/blog/2025/st_petersburg/"><![CDATA[<p>I was recently trying to explain the St. Petersburg paradox to my uncle from memory during a walk in the park. The setup is a deceptively simple coin-tossing game.</p> <blockquote> <p>A fair coin is flipped until it lands heads for the first time. The payout is determined by the number of flips it takes. If heads appears on the 1st toss, you win $2. If on the 2nd, $4. If on the 3rd, $8, and so on, with the payout doubling each time.</p> </blockquote> <p><strong>The question is: What is a fair price to pay to play this game?</strong></p> <p>The mathematical answer for the expected payoff is famously, and confusingly, infinite. The calculation sums the probability of each outcome multiplied by its payout:</p> \[\mathbb{E}[X] = \left(\frac{1}{2}\right)\cdot \$2 + \left(\frac{1}{4}\right)\cdot \$4 + \left(\frac{1}{8}\right)\cdot \$8 + \dots = \sum_{n=1}^{\infty} \frac{1}{2^n} \cdot 2^n = \$1 + \$1 + \$1 + \dots = \infty\] <p>A purely “rational” actor, according to classical theory, should be willing to pay any finite price to play. Yet, intuitively, no one would. Most people would only offer a few dollars. As I explained this, I found myself questioning the standard explanation.</p> <h2 id="the-classical-solution-and-its-context">The Classical “Solution” and Its Context</h2> <p>The classic solution, proposed by <a href="https://psych.fullerton.edu/mbirnbaum/psych466/articles/bernoulli_econometrica.pdf">Daniel Bernoulli in 1738</a>, is that people don’t value money linearly. The “utility” of an extra dollar diminishes as you get wealthier. This <strong>diminishing marginal utility</strong>, often modeled with a logarithmic function, makes the <em>expected utility</em> of the game finite, justifying a small price.</p> <p>Utility theory gives a coherent normative answer and can be derived axiomatically from <a href="https://www.princeton.edu/~wbialek/rome/refs/kelly_56.pdf">Kelly-growth</a> or isoelastic preference arguments. It remains the benchmark in modern finance. However, it is not the only coherent answer. My intuition suggested a different approach:</p> <blockquote> <p>People subconsciously know they have <strong>finite wealth</strong>.</p> </blockquote> <p>To have a real shot at the astronomical prizes that create the infinite expectation (provided the casino has unbounded credit, which is itself unrealistic), you’d need to survive a long string of losses. Most people know their starting stake would be gone long before that lucky streak arrived.</p> <h2 id="a-simulation-and-the-risk-landscape">A Simulation and the Risk Landscape</h2> <p>To test this intuition, I wrote a simulation. The crucial rule, which defines how real people must play, is that <strong>winnings cannot be used to fund more plays</strong>. You arrive with a fixed stake, and when that money is gone, your game is over. This “Stake-Based Model” directly tests the idea that your starting capital is the most important constraint.</p> <p>The goal was to create a landscape of risk, showing how a player’s chance of success changes across different starting stakes and entry prices.</p> <details> <summary>Click to see the simulation code</summary> <pre><code class="language-python">import random
import numpy as np

def simulate_stake_play(start_stake, entry_price):
    stake = float(start_stake)
    net_winnings = 0.0
    
    while stake &gt;= entry_price:
        stake -= entry_price
        
        # Flip until heads
        flips = 1
        while random.random() &gt; 0.5:
            flips += 1
        
        payout = 2**flips
        net_winnings += payout
    
    return net_winnings + stake

def generate_results_table():
    entry_prices = [6, 8, 10, 13]
    wealth_levels = [64, 256, 1024, 8192]
    n_sims = 50_000

    header = "| Starting Stake | " + " | ".join([f"Price = ${p}" for p in entry_prices]) + " |"
    separator = "|:---" + "|:---" * len(entry_prices) + "|"
    print("## Simulation Results: Chance of Breaking Even or Profiting\n")
    print(header)
    print(separator)

    for wealth in wealth_levels:
        row = [f"**${wealth:,}**"]
        
        for price in entry_prices:
            if wealth &lt; price:
                row.append("N/A")
                continue

            outcomes = [simulate_stake_play(wealth, price) for _ in range(n_sims)]
            success_rate = sum(1 for outcome in outcomes if outcome &gt;= wealth) / n_sims
            row.append(f"{success_rate * 100:.1f}%")

        print("| " + " | ".join(row) + " |")

if __name__ == "__main__":
    generate_results_table()
</code></pre> </details> <p>Running this code produces a table that tells a clear story:</p> <h2 id="simulation-results-chance-of-breaking-even-or-profiting">Simulation Results: Chance of Breaking Even or Profiting</h2> <table> <thead> <tr> <th style="text-align: left">Starting Stake</th> <th style="text-align: left">Price = $6</th> <th style="text-align: left">Price = $8</th> <th style="text-align: left">Price = $10</th> <th style="text-align: left">Price = $13</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>$64</strong></td> <td style="text-align: left">49.9%</td> <td style="text-align: left">31.2%</td> <td style="text-align: left">21.6%</td> <td style="text-align: left">14.5%</td> </tr> <tr> <td style="text-align: left"><strong>$256</strong></td> <td style="text-align: left">73.7%</td> <td style="text-align: left">45.9%</td> <td style="text-align: left">31.0%</td> <td style="text-align: left">19.1%</td> </tr> <tr> <td style="text-align: left"><strong>$1,024</strong></td> <td style="text-align: left">95.6%</td> <td style="text-align: left">67.4%</td> <td style="text-align: left">44.0%</td> <td style="text-align: left">26.1%</td> </tr> <tr> <td style="text-align: left"><strong>$8,192</strong></td> <td style="text-align: left">100.0%</td> <td style="text-align: left">98.0%</td> <td style="text-align: left">75.6%</td> <td style="text-align: left">41.1%</td> </tr> </tbody> </table> <p>This table vividly confirms the initial intuition. Reading across any row, the chance of profiting plummets as the price increases. Reading down any column, a larger stake dramatically improves a player’s odds at a fixed price. The game is clearly not the same for everyone.</p> <h2 id="the-world-of-ergodicity-economics">The World of Ergodicity Economics</h2> <p>This line of thinking led me to the work of physicist <a href="https://arxiv.org/abs/1011.4404"><strong>Ole Peters</strong></a> and the field of <strong>ergodicity economics</strong>. It turns out there was a formal name for my intuition. The problem wasn’t psychology; it was that economists were using the wrong kind of average.</p> <ol> <li> <p><strong>Ensemble Average:</strong> This is the standard “expected value” (\(\mathbb{E}[X]\)). It’s the average outcome if a million people played the game in parallel universes <em>at the same time</em>. This is what gives the result of infinity.</p> </li> <li> <p><strong>Time Average:</strong> This is the average outcome for <em>one person</em> playing the game over and over through time. This is what we actually experience in life.</p> </li> </ol> <p>For many systems, these averages are the same (a property called ergodicity). But for a game like St. Petersburg, they are wildly different. Ergodicity economics argues that a rational person should optimize for the time average. This means maximizing the long-term <strong>growth rate</strong> of their wealth, which, for multiplicative processes, is equivalent to maximizing the expected change in the logarithm of wealth.</p> <h3 id="the-exact-growth-neutral-price">The Exact Growth-Neutral Price</h3> <p>A “fair” price <code class="language-plaintext highlighter-rouge">c</code> is one that makes the game “growth-neutral.” The expected change in your log-wealth should be zero. For a player starting with total wealth <code class="language-plaintext highlighter-rouge">W</code> who <strong>reinvests their winnings</strong> (where <code class="language-plaintext highlighter-rouge">c ≤ W</code> is required), we can express this with the following exact equation:</p> \[\mathbb{E}[\Delta \log(W)] = \sum_{n=1}^{\infty} p_n \cdot \log(W - c + \text{payout}_n) - \log(W) = 0\] <p>For our game, where the probability \(p_n = 1/2^n\) and the payout is \(2^n\) (assuming $1 as our monetary unit for the first head), this becomes:</p> \[\sum_{n=1}^{\infty} \frac{1}{2^n} \log(W - c + 2^n) = \log(W)\] <p>This equation is the formal, complete expression for the fair price in a compounding game. However, it cannot be solved algebraically to isolate <code class="language-plaintext highlighter-rouge">c</code>.</p> <h3 id="approximate-rule">Approximate Rule</h3> <p>To find a simple, usable rule, we can derive an approximation. The key insight is that the fair price <code class="language-plaintext highlighter-rouge">c</code> should be related to the number of coin flips required to get a payout that is on the same order of magnitude as your entire wealth, <code class="language-plaintext highlighter-rouge">W</code>. Let’s say this “crossover” happens after <code class="language-plaintext highlighter-rouge">k</code> flips, making the payout <code class="language-plaintext highlighter-rouge">2^k</code>. The central assumption for the approximation is that at the fair price, this transformative payout should be roughly equal to your wealth:</p> \[W \approx 2^k\] <p>This powerful relationship can be solved for <code class="language-plaintext highlighter-rouge">k</code> by taking the base-2 logarithm of both sides:</p> \[\log_2(W) \approx \log_2(2^k) \implies \log_2(W) \approx k\] <p>However, solving the exact time-average equation numerically reveals that a more accurate approximation for the fair price <code class="language-plaintext highlighter-rouge">c</code> a person with wealth <code class="language-plaintext highlighter-rouge">W</code> should pay is:</p> \[c^*(W) \approx \log_2(W) + 1\] <p>The additional +1 term comes from the leading-order expansion of the exact equation and shrinks only logarithmically with wealth. This single equation resolves the paradox cleanly. The price is not infinite; it is a finite number that depends directly on a player’s wealth.</p> <h2 id="empirical-validation">Empirical Validation</h2> <p>The <code class="language-plaintext highlighter-rouge">log₂(W) + 1</code> formula gives us the fair price for the theoretical compounding game. But what is the true fair price for the games we simulated? We created code to find the exact price that gives a 50/50 chance of success for two different scenarios:</p> <ol> <li> <p><strong>Stake-Based Model:</strong> Our realistic simulation where winnings are kept separate.</p> </li> <li> <p><strong>Total Ruin Model:</strong> An even riskier version where running out of stake means you forfeit all winnings.</p> </li> </ol> <p>By comparing the fair prices for these models, we can precisely measure the “risk premium” associated with different rules.</p> <details> <summary>Click to see the price-finding code</summary> <pre><code class="language-python">import random
import math

def simulate_stake_play(start_stake, entry_price):
    stake = float(start_stake)
    winnings = 0.0
    
    while stake &gt;= entry_price:
        stake -= entry_price
        flips = 1
        while random.random() &gt; 0.5:
            flips += 1
        winnings += 2**flips
    
    return winnings + stake

def simulate_total_ruin_play(start_stake, entry_price):
    stake = float(start_stake)
    winnings = 0.0
    
    while stake &gt;= entry_price:
        stake -= entry_price
        flips = 1
        while random.random() &gt; 0.5:
            flips += 1
        winnings += 2**flips
    
    # Total ruin: lose everything if you don't break even
    return winnings + stake if winnings &gt;= start_stake else 0.0

def find_fair_price(game_func, wealth, target_success=0.5, tolerance=0.001):
    low = max(0.01, math.log2(wealth) - 3)
    high = math.log2(wealth)
    
    for _ in range(30):
        price = (low + high) / 2
        if price &lt;= 0:
            low = 0.01
            continue
            
        outcomes = [game_func(wealth, price) for _ in range(10000)]
        success_rate = sum(1 for x in outcomes if x &gt;= wealth) / len(outcomes)
        
        if abs(success_rate - target_success) &lt; tolerance:
            return price
        elif success_rate &gt; target_success:
            low = price
        else:
            high = price
    
    return (low + high) / 2

def compare_fair_prices():
    wealth_levels = [64, 256, 1024, 8192]
    
    print("| Starting Stake | Exact Compounding Price | Fair Price (Stake-Based) | Fair Price (Total Ruin) |")
    print("|:---|:---|:---|:---|")
    
    for w in wealth_levels:
        exact = math.log2(w) + 1
        stake_price = find_fair_price(simulate_stake_play, w)
        ruin_price = find_fair_price(simulate_total_ruin_play, w)
        
        print(f"| **${w:,}** | ${exact:.2f} | ${stake_price:.2f} | ${ruin_price:.2f} |")

if __name__ == "__main__":
    compare_fair_prices()
</code></pre> </details> <h2 id="final-results-the-risk-premium-quantified">Final Results: The Risk Premium Quantified</h2> <table> <thead> <tr> <th style="text-align: left">Starting Stake</th> <th style="text-align: left">Exact Compounding Price</th> <th style="text-align: left">Fair Price (Stake-Based)</th> <th style="text-align: left">Fair Price (Total Ruin)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>$64</strong></td> <td style="text-align: left">$7.00</td> <td style="text-align: left">$5.97</td> <td style="text-align: left">$5.82</td> </tr> <tr> <td style="text-align: left"><strong>$256</strong></td> <td style="text-align: left">$9.00</td> <td style="text-align: left">$7.63</td> <td style="text-align: left">$7.53</td> </tr> <tr> <td style="text-align: left"><strong>$1,024</strong></td> <td style="text-align: left">$11.00</td> <td style="text-align: left">$9.39</td> <td style="text-align: left">$9.31</td> </tr> <tr> <td style="text-align: left"><strong>$8,192</strong></td> <td style="text-align: left">$14.00</td> <td style="text-align: left">$11.96</td> <td style="text-align: left">$11.96</td> </tr> </tbody> </table> <p>The results clearly show the hierarchy of risk. The exact compounding price (where winnings can be reinvested) represents the theoretical upper bound for a growth-neutral game. As game rules become riskier, fair prices decrease accordingly.</p> <p>Because compounding reduces risk by allowing reinvestment of winnings, the exact compounding price acts as an <strong>upper bound</strong> above both the stake-based and total ruin prices. Our stake-based model, being more realistic but riskier than full compounding, commands a lower price. The brutal total ruin model requires the lowest price of all.</p> <p>We can now quantify the risk premium precisely. For a player with $256:</p> <ul> <li>The premium for separating stake from winnings is <code class="language-plaintext highlighter-rouge">$9.00 - $7.63 = $1.37</code>.</li> <li>The additional premium for facing total ruin is <code class="language-plaintext highlighter-rouge">$7.63 - $7.53 = $0.10</code>.</li> </ul> <h2 id="the-shrinking-risk-premium">The Shrinking Risk Premium</h2> <p>A careful examination of the results reveals a fascinating pattern. The risk premium between the stake-based and total ruin models shrinks as wealth increases:</p> <ul> <li><strong>$64</strong>: $0.15 premium (2.5% of stake price)</li> <li><strong>$256</strong>: $0.10 premium (1.3% of stake price)</li> <li><strong>$1,024</strong>: $0.08 premium (0.9% of stake price)</li> <li><strong>$8,192</strong>: Essentially zero premium</li> </ul> <p>This convergence is not due to the fat-tailed nature of the payouts, but rather to a fundamental principle from probability theory: <strong>gambler’s ruin</strong>.</p> <p>The total ruin penalty only applies in a very specific scenario: you must go bust (your stake runs out) <em>and</em> your cumulative winnings must still be less than your initial wealth. As your wealth increases relative to the entry price, the probability of this specific event shrinks dramatically according to <a href="https://en.wikipedia.org/wiki/Gambler%27s_ruin">classical gambler’s ruin theory</a>.</p> <p>The probability of ruin before hitting a significant win scales roughly as <code class="language-plaintext highlighter-rouge">c/W</code> (entry price divided by wealth):</p> <ul> <li><strong>$64 wealth, $6 price</strong>: ~9.4% chance of the penalty scenario</li> <li><strong>$8,192 wealth, $12 price</strong>: ~0.1% chance of the penalty scenario</li> </ul> <p>As this probability approaches zero, the “insurance value” of keeping winnings separate from stake becomes negligible. Both models converge to essentially the same expected outcome because the scenario that differentiates them almost never happens.</p> <p>This insight reveals an important distinction in the mathematics:</p> <ul> <li><strong>Fat tails</strong> explain why the <code class="language-plaintext highlighter-rouge">log₂(W)</code> pricing formula works and why wealth matters for fair pricing</li> <li><strong>Gambler’s ruin probability</strong> explains why different rule variants converge at high wealth levels</li> </ul> <p>The convergence demonstrates that at sufficient wealth levels, the specific mechanics of how losses are handled become less important than simply having enough capital to survive until the inevitable large payout arrives.</p> <h2 id="conclusion">Conclusion</h2> <p>The paradox dissolves once we specify the averaging procedure. The ensemble average is blind to the realities of time, sequence, and survival that every real person faces.</p> <p>You don’t need a psychological theory of “utility” to explain why people don’t pay much to play. You just need to model the game they are actually playing, one with a finite stake and a real risk of ruin. The value of the game is not a universal constant but a dynamic function of your personal circumstances and the specific rules under which you must operate—a conclusion that ergodicity economics formalizes and our simulation empirically verifies.</p>]]></content><author><name></name></author><category term="mathematics"/><category term="economics"/><category term="computer science"/><category term="ergodicity"/><category term="utility theory"/><category term="decision theory"/><summary type="html"><![CDATA[How an old paradox reveals a deep flaw in classical economics and points the way to a more realistic model of human decision-making.]]></summary></entry><entry><title type="html">The Button Game: When Should You Stop for the Best Prize?</title><link href="https://www.kparvanov.com/blog/2025/button_game/" rel="alternate" type="text/html" title="The Button Game: When Should You Stop for the Best Prize?"/><published>2025-06-24T19:00:00+00:00</published><updated>2025-06-24T19:00:00+00:00</updated><id>https://www.kparvanov.com/blog/2025/button_game</id><content type="html" xml:base="https://www.kparvanov.com/blog/2025/button_game/"><![CDATA[<h2 id="an-interview-puzzle">An Interview Puzzle</h2> <p>While preparing for an upcoming technical interview, I came across a probability puzzle that, at first glance, seems like a simple game of chance, but quickly reveals layers of strategic depth. The problem was as follows:</p> <blockquote> <p>An agent can press a button up to 10 times. Each press generates a random monetary value, drawn from a continuous uniform distribution on the interval <code class="language-plaintext highlighter-rouge">[0, $100,000]</code>. After each press, the agent can either stop and keep the current value, ending the game, or discard the value and continue to the next press. If the agent completes all 10 presses, they must accept the value from the 10th press.</p> </blockquote> <p><strong>The question is: What is the optimal strategy to maximize the expected payoff?</strong></p> <p>My initial intuition was to work backward from the end. But after solving it, I became more interested in the bigger picture: what kind of problem is this, and what mathematical principles guarantee that the solution is truly optimal?</p> <h2 id="zooming-out-the-world-of-optimal-stopping">Zooming Out: The World of Optimal Stopping</h2> <p>This puzzle is not a one-off brain teaser; it’s a classic example of a <strong>finite-horizon optimal stopping problem</strong>. This class of problems is fundamental in fields like finance, economics, and computer science, dealing with the challenge of choosing the perfect time to take a particular action to maximize a reward or minimize a cost.</p> <p>To analyze such problems rigorously, we model them as <a href="https://en.wikipedia.org/wiki/Markov_decision_process"><strong>Markov Decision Processes (MDPs)</strong></a>. An MDP is a mathematical framework for decision-making in situations where outcomes are partly random and partly under the control of a decision-maker. Our game fits this framework perfectly:</p> <ol> <li><strong>States:</strong> The state at any point is simply the number of presses remaining, \(n\).</li> <li><strong>Actions:</strong> In any state (where \(n &gt; 1\)), our actions are to <code class="language-plaintext highlighter-rouge">stop</code> or <code class="language-plaintext highlighter-rouge">continue</code>.</li> <li><strong>The Markov Property:</strong> Most importantly, the game is “memoryless.” The decision we make with \(n\) presses left depends only on the current state and the number we just drew, not on the history of previously rejected values. Each press is an independent event.</li> </ol> <p>Recognizing the problem as an MDP is the first step, as it allows us to bring a powerful toolkit to bear on finding the solution: the theory of dynamic programming.</p> <h2 id="dynamic-programming-and-bellmans-principle">Dynamic Programming and Bellman’s Principle</h2> <p>The cornerstone for solving MDPs is <strong>dynamic programming</strong>, a method for breaking down a complex problem into a sequence of simpler, nested subproblems. The philosophical foundation of this method is <a href="https://en.wikipedia.org/wiki/Bellman_equation#Bellman's_principle_of_optimality"><strong>Bellman’s Principle of Optimality</strong></a>.</p> <blockquote> <p><strong>Bellman’s Principle of Optimality</strong> states that an optimal policy has the property that whatever the initial state and first decision are, the remaining decisions must constitute an optimal policy for the subproblem starting from the new state.</p> </blockquote> <p>In simpler terms, a perfect overall plan is composed of a series of perfect sub-plans. To play the 10-press game optimally, our strategy for the remaining 9 presses (should we continue) must also be the optimal strategy for a 9-press game.</p> <p>This principle is captured mathematically in the <a href="https://en.wikipedia.org/wiki/Bellman_equation"><strong>Bellman Equation</strong></a>. For our type of optimal stopping problem, the equation takes the following form. If we let \(V_n\) be the maximum expected payoff we can get with \(n\) presses remaining, then:</p> \[V_n = \mathbb{E}[\max(X_n, V_{n-1})]\] <p>This elegant equation is the blueprint for our strategy. It says the value of being in a state with \(n\) presses left is the expected value of the <em>best possible choice</em>: either the random value \(X_n\) we just drew, or the value \(V_{n-1}\) we expect to get if we discard the current draw and play optimally from the next state.</p> <h2 id="applying-the-theory-to-solve-the-puzzle">Applying the Theory to Solve the Puzzle</h2> <p>With this theoretical framework, we can now solve the interview puzzle methodically using <strong>backward induction</strong>. For clarity, let’s set \(C = 100,000\).</p> <p><strong>Step 1: Solve the Base Case (\(n=1\))</strong> We start at the end. With only one press left, we must accept the outcome. The expected value is the mean of the uniform distribution: \(V_1 = \mathbb{E}[X] = \frac{C}{2} = \$50,000\)</p> <p><strong>Step 2: Derive the General Recurrence Relation</strong> For any stage \(n \ge 2\), the decision rule is to stop if the draw \(x\) is greater than the continuation value, \(V_{n-1}\). We can find the value \(V_n\) by calculating the expectation from the Bellman Equation:</p> \[V_n = \mathbb{E}[\max(X, V_{n-1})] = \int_{0}^{C} \max(x, V_{n-1}) \frac{dx}{C}\] <p>We split the integral at the threshold \(V_{n-1}\), since the value of \(\max(x, V_{n-1})\) changes at that point:</p> \[V_n = \frac{1}{C} \left[ \int_{0}^{V_{n-1}} V_{n-1} \,dx + \int_{V_{n-1}}^{C} x \,dx \right]\] <p>Now, we solve the integrals:</p> \[V_n = \frac{1}{C} \left[ V_{n-1} [x]_0^{V_{n-1}} + \left[\frac{x^2}{2}\right]_{V_{n-1}}^C \right]\] \[V_n = \frac{1}{C} \left[ V_{n-1}(V_{n-1} - 0) + \left(\frac{C^2}{2} - \frac{V_{n-1}^2}{2}\right) \right]\] \[V_n = \frac{1}{C} \left[ V_{n-1}^2 + \frac{C^2 - V_{n-1}^2}{2} \right] = \frac{2V_{n-1}^2 + C^2 - V_{n-1}^2}{2C}\] <p>This simplifies to the clean, general recurrence relation we can use for all steps:</p> \[V_n = \frac{V_{n-1}^2 + C^2}{2C}\] <p>Using this formula, we can now iterate backward from our known base case (\(V_1 = 50,000\)) to generate the complete optimal strategy. For instance:</p> <ul> <li><strong>For n=2:</strong> \(V_2 = \frac{(50000)^2 + (100000)^2}{2 \cdot 100000} = \$62,500\)</li> <li><strong>For n=3:</strong> \(V_3 = \frac{(62500)^2 + (100000)^2}{2 \cdot 100000} \approx \$69,531\)</li> </ul> <p>Repeating this process for all 10 stages gives us the following policy:</p> <table> <thead> <tr> <th style="text-align: left">Presses Left (n)</th> <th style="text-align: left">Threshold to Stop (\(V_{n-1}\))</th> <th style="text-align: left">Expected Payoff (\(V_n\))</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">10</td> <td style="text-align: left">$84,982</td> <td style="text-align: left">$86,110</td> </tr> <tr> <td style="text-align: left">9</td> <td style="text-align: left">$83,645</td> <td style="text-align: left">$84,982</td> </tr> <tr> <td style="text-align: left">8</td> <td style="text-align: left">$82,030</td> <td style="text-align: left">$83,645</td> </tr> <tr> <td style="text-align: left">7</td> <td style="text-align: left">$80,038</td> <td style="text-align: left">$82,030</td> </tr> <tr> <td style="text-align: left">6</td> <td style="text-align: left">$77,508</td> <td style="text-align: left">$80,038</td> </tr> <tr> <td style="text-align: left">5</td> <td style="text-align: left">$74,173</td> <td style="text-align: left">$77,508</td> </tr> <tr> <td style="text-align: left">4</td> <td style="text-align: left">$69,531</td> <td style="text-align: left">$74,173</td> </tr> <tr> <td style="text-align: left">3</td> <td style="text-align: left">$62,500</td> <td style="text-align: left">$69,531</td> </tr> <tr> <td style="text-align: left">2</td> <td style="text-align: left">$50,000</td> <td style="text-align: left">$62,500</td> </tr> <tr> <td style="text-align: left">1</td> <td style="text-align: left">(Must accept draw)</td> <td style="text-align: left">$50,000</td> </tr> </tbody> </table> <p><em>Note: Dollar values are rounded to the nearest dollar.</em></p> <h2 id="empirical-validation-with-python-simulation">Empirical Validation with Python Simulation</h2> <p>As a final check, a Monte Carlo simulation confirms that this strategy works in practice.</p> <h3 id="implementation">Implementation</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">random</span>

<span class="k">def</span> <span class="nf">calculate_thresholds</span><span class="p">(</span><span class="n">max_presses</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mi">100000</span><span class="p">):</span>
    <span class="n">V</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">V</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">C</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">max_presses</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">V_n_minus_1</span> <span class="o">=</span> <span class="n">V</span><span class="p">[</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">V</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">V_n_minus_1</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">C</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">C</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">V</span>

<span class="k">def</span> <span class="nf">play_one_game</span><span class="p">(</span><span class="n">thresholds</span><span class="p">,</span> <span class="n">max_presses</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mi">100000</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">presses_left</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_presses</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">current_draw</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">presses_left</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">current_draw</span>
        <span class="n">threshold</span> <span class="o">=</span> <span class="n">thresholds</span><span class="p">[</span><span class="n">presses_left</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">current_draw</span> <span class="o">&gt;=</span> <span class="n">threshold</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">current_draw</span>
    <span class="k">return</span> <span class="mi">0</span> 

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="n">MAX_PRESSES</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">C</span> <span class="o">=</span> <span class="mi">100000</span>
    <span class="n">NUM_SIMULATIONS</span> <span class="o">=</span> <span class="mi">1_000_000</span>
    <span class="n">thresholds</span> <span class="o">=</span> <span class="nf">calculate_thresholds</span><span class="p">(</span><span class="n">MAX_PRESSES</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Theoretical Expected Payoff: $</span><span class="si">{</span><span class="n">thresholds</span><span class="p">[</span><span class="n">MAX_PRESSES</span><span class="p">]</span><span class="si">:</span><span class="p">,.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">total_payoff</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="nf">play_one_game</span><span class="p">(</span><span class="n">thresholds</span><span class="p">,</span> <span class="n">MAX_PRESSES</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">NUM_SIMULATIONS</span><span class="p">))</span>
    <span class="n">average_payoff</span> <span class="o">=</span> <span class="n">total_payoff</span> <span class="o">/</span> <span class="n">NUM_SIMULATIONS</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Simulated Average Payoff (</span><span class="si">{</span><span class="n">NUM_SIMULATIONS</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s"> games): $</span><span class="si">{</span><span class="n">average_payoff</span><span class="si">:</span><span class="p">,.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="nf">main</span><span class="p">()</span>
</code></pre></div></div> <h3 id="results">Results</h3> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Theoretical Expected Payoff: $86,109.82

Simulated Average Payoff (1,000,000 games): $86,112.80
</code></pre></div></div> <p>The simulation beautifully converges to the theoretical result.</p> <h2 id="why-the-strategy-is-optimal">Why the Strategy is Optimal</h2> <p>This brings us to the decisive question: why is this threshold rule not merely good, but provably <strong>optimal</strong>? The answer rests on two foundational results.</p> <p>Because the game is a finite-horizon Markov decision process, it terminates after ten presses. For such problems:</p> <ul> <li><a href="https://en.wikipedia.org/wiki/Bellman_equation#Bellman's_principle_of_optimality"><strong>Bellman’s Principle of Optimality</strong></a> guarantees that an overall best plan must embed a best plan for every remaining sub-game.</li> <li>The corresponding <a href="https://en.wikipedia.org/wiki/Bellman_equation"><strong>Bellman Equation</strong></a>: \(V_n = \mathbb{E}\!\bigl[\max\!\bigl(X_n,\;V_{n-1}\bigr)\bigr],\)</li> </ul> <p>solved backward from \(V_1 = C/2\), delivers the unique fixed point of the dynamic-programming operator.</p> <p>The fixed point yields the concrete rule <strong>“stop when the draw \(X \ge V_{n-1}\).”</strong> Because the Bellman operator is a contraction, no alternative policy, no matter how elaborate, can exceed the expected payoff \(V_{10}\). Each comparison of the current draw \(x\) with the continuation value \(V_{n-1}\) therefore constitutes the globally optimal decision at that step (technically, the threshold rule is the first hitting time of the <a href="https://en.wikipedia.org/wiki/Snell_envelope"><em>Snell envelope</em></a>, guaranteeing no other stopping rule can beat it).</p>]]></content><author><name></name></author><category term="mathematics"/><category term="computer science"/><category term="data analysis"/><category term="optimal stopping"/><category term="backward induction"/><category term="dynamic programming"/><category term="probability"/><summary type="html"><![CDATA[An interesting interview question about optimal stopping.]]></summary></entry><entry><title type="html">Detecting Regime Shifts in SP500 Stocks Using PCA and Sparse PCA</title><link href="https://www.kparvanov.com/blog/2024/distill/" rel="alternate" type="text/html" title="Detecting Regime Shifts in SP500 Stocks Using PCA and Sparse PCA"/><published>2024-10-13T00:00:00+00:00</published><updated>2024-10-13T00:00:00+00:00</updated><id>https://www.kparvanov.com/blog/2024/distill</id><content type="html" xml:base="https://www.kparvanov.com/blog/2024/distill/"><![CDATA[<h2 id="introduction-dimensionality-reduction-and-financial-data-analysis">Introduction: Dimensionality Reduction and Financial Data Analysis</h2> <p>This project applies Principal Component Analysis (PCA) and Sparse PCA to analyze the S&amp;P 500 stock market. By leveraging these dimensionality reduction techniques on high-frequency trading data, the aim is to uncover hidden structures, identify regime shifts, and understand key factors driving stock returns.</p> <p>PCA is crucial in financial data analysis as it can reduce the dimensionality of large datasets while preserving essential information. This allows for the identification of primary factors influencing market behavior, valuable for risk management and portfolio optimization.</p> <p>The dataset initially comprised all 503 stocks from the S&amp;P 500 index, sampled at 2-minute intervals over 31 trading days from August 8 to September 19, 2024, using the Yahoo Finance API. This high-frequency data captures intraday patterns and short-term market dynamics often missed in lower frequency data.</p> <p>Through standard PCA and Sparse PCA, the project aims to identify the main components driving market variance and enhance their interpretability. Sparse PCA particularly helps pinpoint key stocks most influential in each component.</p> <p>The analysis explores sector-specific patterns, correlations with major ETF factors, and the evolution of market regimes over time, offering potentially valuable insights for both academic research and industry practice in quantitative finance. Finally, the project is also an opportunity for me to practice and demonstrate my skills in the field of Machine Learning.</p> <h2 id="data-preparation">Data Preparation</h2> <h3 id="data-verification-and-consistency">Data Verification and Consistency</h3> <p>The initial dataset included all 503 S&amp;P 500 stocks and 20 ETFs, covering the period from August 8 to September 19, 2024. This data was sampled at 2-minute intervals using the Yahoo Finance API, resulting in 31 trading days of high-frequency data.</p> <p>To ensure data consistency, I focused on standard market hours, specifically 9:30 AM to 4:00 PM EST. This approach eliminates potential discrepancies or anomalies that might occur in pre-market or after-hours trading, providing a more reliable basis for analysis.</p> <h3 id="data-cleaning-and-preprocessing">Data Cleaning and Preprocessing</h3> <p>The data cleaning process involved several critical steps:</p> <ol> <li> <p><strong>Handling Missing Data</strong>: I implemented a forward+backward-filling strategy with a limit of 5 intervals to address short gaps in the data. This approach preserves the temporal structure of the data while avoiding the introduction of artificial patterns.</p> </li> <li> <p><strong>Asset Filtering</strong>: To maintain data quality, I removed assets with excessive missing data. Specifically, any asset with more than 20% missing data was excluded from the analysis. This resulted in the retention of 457 stocks out of the initial 503, as some stocks lacked sufficient data from the Yahoo Finance API.</p> </li> <li> <p><strong>Timestamp Synchronization</strong>: All stock data was aligned to a common time index, ensuring that price movements are compared at precisely the same moments across the entire dataset.</p> </li> <li> <p><strong>Return Calculation</strong>: Instead of working with raw price data, I calculated logarithmic returns. Log returns are preferred in financial analysis as they are additive over time and provide a more accurate representation of percentage changes, especially for high-frequency data.</p> </li> <li> <p><strong>Standardization</strong>: To ensure comparability across stocks with different price levels and volatilities, I standardized the returns. This process involves subtracting the mean and dividing by the standard deviation for each stock’s return series, resulting in a dataset where each stock has a mean of 0 and a standard deviation of 1.</p> </li> </ol> <p>The preprocessing yielded a final dataset of 5,463 valid time points for 457 stocks and 20 ETFs, retaining 96.6% of the combined stock data. This high retention rate ensures data quality while accommodating necessary exclusions due to insufficient data. The resulting cleaned and standardized dataset of log-normalized returns provides a good foundation for PCA and Sparse PCA analyses, allowing for the identification of genuine market patterns and relationships.</p> <h2 id="principal-component-analysis-pca">Principal Component Analysis (PCA)</h2> <p>Principal Component Analysis (PCA) is a dimensionality reduction technique widely used in financial data analysis. It transforms the original dataset into a new coordinate system where the axes (principal components) are ordered by the amount of variance they explain in the data. This section explores three applications of PCA to my dataset of S&amp;P 500 stock returns: Full Period PCA, Sparse PCA, and Sector-based PCA analysis.</p> <h3 id="full-period-pca">Full Period PCA</h3> <p>I applied Full Period PCA to the entire dataset, covering all 457 stocks over the 31-day trading period. Mathematically, PCA seeks to find a set of orthogonal vectors (principal components) that maximize the variance of the projected data.</p> <p>For a data matrix $X$, PCA solves the eigenvalue problem:</p> \[(X^TX)v_i = \lambda_i v_i\] <p>where $v_i$ are the eigenvectors (principal components) and $\lambda_i$ are the corresponding eigenvalues, which represent the amount of variance explained by each component.</p> <p>Note that the components are usually arranged in decreasing order based on the explained variance they have. Thus, PC1 would have the most explained variance, PC2 would have the second most explained variance, etc.</p> <p>Finally, I performed the analysis using sklearn’s PCA implementation, with the number of components determined by the cumulative explained variance ratio. This approach allowed me to identify the key drivers of variance in the stock market during the studied period.</p> <h4 id="interpretation-of-full-period-pca-results">Interpretation of Full Period PCA Results</h4> <p>The first plot generated from the PCA on the full trading period was the cumulative explained variance plot, which is crucial for understanding how many principal components are needed to explain a given percentage of the variance in the data. I observed that capturing 80% of the variance requires 154 components, while 90% requires 255 components—slightly more than half of the total 457 components. This outcome is typical for financial time-series data, likely due to its heavy-tailed distribution. An additional noteworthy observation from the plot below is that the first principal component alone explains nearly 25% of the variance in the data. Traditionally, this component is strongly associated with broad market movement and may capture underlying general trends in stock prices. However, it’s important to recognize that principal components often represent a mix of various factors rather than having singular, clear-cut meanings. As such, a thorough analysis is necessary before attempting to interpret principal components as representing specific economic or market factors.</p> <div class="l-page"> <iframe src="/assets/plotly/RegimeShift/full_period_pca_variance.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed grey;"></iframe> </div> <div class="caption"> Cumulative explained variance plot. The plot shows that 154 components are needed to explain 80% of the variance, and 225 components are required to explain 90%. Plot credit: <a href="https://github.com/parvanovkp/RegimeShift/blob/main/pca_results/full_period_pca_variance.html">Kal Parvanov/GitHub</a> </div> <p>The second, and more intriguing, plot generated was a heatmap illustrating the relationship between the stocks in the dataset and the first 10 principal components. While the heatmap might initially seem overwhelming due to the large number of stocks, it reveals distinct clusters that are either positively or negatively associated with certain principal components. This observation led me to conclude that these stock clusters, likely representing individual sectors, could be significantly contributing to the principal components. If this is the case, it opens the possibility of assigning economic meaning to some components, potentially interpreting them as reflective of specific sectors of the economy. To investigate this further, I decided to apply sparse PCA, which is better suited to reducing noise and isolating the key drivers within each principal component.</p> <div class="l-page"> <iframe src="/assets/plotly/RegimeShift/full_period_pca_heatmap.html" frameborder="0" scrolling="yes" height="500px" width="100%" style="border: 1px dashed grey;"></iframe> </div> <div class="caption"> Heatmap of the contribution of individual stocks to each of the first 10 principal components. Plot credit: <a href="https://github.com/parvanovkp/RegimeShift/blob/main/pca_results/full_period_pca_heatmap.html">Kal Parvanov/GitHub</a> </div> <h3 id="sparse-pca">Sparse PCA</h3> <p>While standard PCA provides valuable insights, its components often involve all input variables, making interpretation challenging. To address this limitation, I turned to Sparse PCA, which introduces sparsity in the principal components, effectively selecting a subset of the most influential stocks for each component.</p> <p>Sparse PCA solves an optimization problem of the form:</p> \[\min_{U,V} \frac{1}{2}||X - UV^T||_F^2 + \alpha||V||_1\] <p>where $U$ are the sparse loadings, $V$ are the components, $\alpha$ is a sparsity parameter, and $||\cdot||_F$ denotes the Frobenius norm.</p> <p>I applied Sparse PCA using sklearn’s SparsePCA implementation, setting the number of components to 10. This approach enhances interpretability by identifying key stocks driving each component, allowing for more focused analysis of market dynamics.</p> <p>The clusters observed in the heatmap during standard PCA suggested that specific groups of stocks, likely from the same sectors, contribute meaningfully to certain principal components. Sparse PCA’s ability to highlight these key contributors while filtering out noise makes it an ideal tool for refining the sector-based analysis. By isolating the most influential stocks, Sparse PCA enhances the interpretability of the components and allows for a more focused exploration of sector-driven market dynamics.</p> <p>Of course, as with all analytical tools, Sparse PCA has both benefits and drawbacks when it comes to its use in financial data analysis:</p> <p>Benefits:</p> <ul> <li><strong>Improved Interpretability</strong>: Unlike standard PCA, where all stocks contribute to each component, Sparse PCA isolates the key stocks driving each principal component. This allows for more intuitive interpretation, as the components are linked to a smaller group of stocks, potentially highlighting sector-specific dynamics or market themes.</li> <li><strong>Reduction of Noise</strong>: By forcing many loadings to zero, Sparse PCA filters out less relevant stocks, reducing the noise and complexity in the interpretation of the components.</li> <li><strong>Sector-based Analysis</strong>: Sparse PCA’s ability to highlight key contributors makes it particularly useful for sector-based analysis. The identification of influential stocks within each component can provide insights into which sectors are driving market movements and how they relate to broader economic factors.</li> </ul> <p>Drawbacks:</p> <ul> <li><strong>Loss of Information</strong>: By focusing only on a subset of stocks, Sparse PCA may lose some of the finer details captured by the more comprehensive components of standard PCA. This can be a trade-off between interpretability and completeness.</li> <li><strong>Sensitivity to Parameter Tuning</strong>: The sparsity of the solution depends on the choice of the $\alpha$ parameter, and selecting the right value is not always straightforward. A high $\alpha$ may oversimplify the model by excluding too many stocks, while a low $\alpha$ may not provide enough sparsity for meaningful interpretation.</li> <li><strong>Computational Complexity</strong>: Sparse PCA can be more computationally intensive than standard PCA, especially when tuning parameters or working with large datasets.</li> </ul> <p>Despite these drawbacks, I found Sparse PCA to be a powerful tool for uncovering the underlying structure of the S&amp;P 500 stock returns, particularly in identifying sector-specific influences and key market drivers.</p> <h3 id="sector-based-pca-analysis">Sector-Based PCA Analysis</h3> <p>To investigate sector-specific patterns in stock returns, I performed a sector-based analysis using the Sparse PCA results. I obtained the sector classification using the yfinance API, with stocks assigned to one of ten merged sector categories:</p> <ol> <li>Technology</li> <li>Healthcare</li> <li>Consumer (Cyclical and Defensive)</li> <li>Industrials</li> <li>Financial Services</li> <li>Energy</li> <li>Communication Services</li> <li>Real Estate</li> <li>Utilities</li> <li>Basic Materials</li> </ol> <p>In this approach, I aggregated the Sparse PCA loadings for stocks within each sector, separating positive and negative loadings. This allowed me to visualize how different sectors contribute to each principal component, providing insights into sector-specific patterns and their influence on overall market movements.</p> <h4 id="interpretation-of-sector-based-pca-analysis-results">Interpretation of Sector-based PCA Analysis Results</h4> <p>The Sparse PCA - Sector Loadings Heatmap provides a nuanced view of how different sectors contribute to each principal component, offering valuable insights into sector-specific patterns and their influence on overall market movements.</p> <div class="l-page"> <iframe src="/assets/plotly/RegimeShift/sparse_pca_sector_heatmaps.html" frameborder="0" scrolling="yes" height="650px" width="100%" style="border: 1px dashed grey;"></iframe> </div> <div class="caption"> Sector Loadings Heatmap: Sparse PCA Component Contributions Across Sectors. Plot credit: <a href="https://github.com/parvanovkp/RegimeShift/blob/main/pca_results/sparse_pca_sector_heatmaps.html">Kal Parvanov/GitHub</a> </div> <p>Analyzing the heatmap reveals several key observations:</p> <p><strong>Sector-specific Contributions:</strong></p> <table> <thead> <tr> <th>PC</th> <th>Positive Influence</th> <th>Negative Influence</th> <th>Potential Interpretation</th> </tr> </thead> <tbody> <tr> <td>1</td> <td>Industrials (36.6%)</td> <td>Consumer (48%)</td> <td>Production vs. consumption dynamic</td> </tr> <tr> <td>2</td> <td>Utilities (51.1%), Consumer (19.2%)</td> <td>Technology (37.3%)</td> <td>Defensive sectors vs. growth-oriented tech</td> </tr> <tr> <td>3</td> <td>Energy (45.1%)</td> <td>Consumer (35.5%), Technology (26.4%)</td> <td>Traditional energy vs. tech-driven consumer trends</td> </tr> <tr> <td>4</td> <td>Consumer (51.1%)</td> <td>Technology (28%), Consumer (16.7%)</td> <td>Intra-consumer dynamics and tech influence</td> </tr> <tr> <td>5</td> <td>Real Estate (50%)</td> <td>Consumer (43.2%), Technology (18.1%)</td> <td>Property market vs. consumer spending and tech</td> </tr> <tr> <td>7</td> <td>Financial Services (57.6%)</td> <td>Healthcare (21.8%), Technology (20.6%), Consumer (17.4%)</td> <td>Financial sector performance vs. healthcare and tech innovations</td> </tr> <tr> <td>9</td> <td>Technology (55%)</td> <td>Financial Services (25.4%), Consumer (23.4%), Healthcare (20%)</td> <td>Tech disruption vs. traditional finance and consumer sectors</td> </tr> </tbody> </table> <p><strong>Dominant Sectors per Component (Rest of Components):</strong></p> <table> <thead> <tr> <th>PC</th> <th>Positive Influence</th> <th>Negative Influence</th> </tr> </thead> <tbody> <tr> <td>6</td> <td>Consumer (34.3%), Industrials (18.5%)</td> <td>Healthcare (22.9%), Financials (20.8%)</td> </tr> <tr> <td>8</td> <td>Consumer (30.4%), Healthcare (28.2%)</td> <td>Industrials (22.3%), Technology (18.6%)</td> </tr> <tr> <td>10</td> <td>Healthcare (34.7%), Consumer (25.7%)</td> <td>Consumer (31.7%), Financial Services (17.1%), Technology (13.6%)</td> </tr> </tbody> </table> <ol> <li> <p><strong>Unique Insights:</strong></p> <ul> <li>Energy sector shows concentrated influence in PC3, suggesting impact specific to certain economic conditions.</li> <li>Real Estate has strong positive influence in PC5 but minimal impact elsewhere, indicating isolated movements.</li> <li>Technology consistently appears as a negative influence, suggesting its role as a disruptive force.</li> </ul> </li> <li> <p><strong>Cross-Component Patterns:</strong></p> <ul> <li>Consumer sector shows complex, varying influence across components, reflecting its multifaceted role in the economy.</li> <li>Healthcare demonstrates diverse impact, with positive influence in later components (PC8, PC10) but negative in PC6.</li> <li>Financial Services sector’s loading pattern suggests sensitivity to specific economic factors like interest rates or regulatory changes.</li> </ul> </li> <li> <p><strong>Sector Stability and Variability:</strong></p> <ul> <li>Utilities show concentrated strong positive influence in PC2, potentially representing a stable, defensive factor.</li> <li>Technology and Consumer sectors frequently appear across components, often in opposition, indicating their central role in market dynamics.</li> </ul> </li> </ol> <p>This sector-based analysis provides insights not easily discernible from individual stock analysis or full dataset PCA, highlighting how different economic sectors interact and contribute to overall market movements. The clear sector-specific patterns demonstrate the effectiveness of this approach in identifying key drivers of stock market returns.</p> <p>The varying roles of sectors across components underscore the complexity of market dynamics and the importance of considering multiple factors in stock market analysis. This analysis reveals not just important sectors for each component, but also how they might work in opposition, providing a richer picture of the complex interplay between market segments.</p> <p>It is important to note that these results are specific to the selected time window and may not generalize to other periods. Market dynamics can change over time, and the sector contributions observed here may differ under different economic conditions.</p> <h2 id="correlation-with-etf-factors">Correlation with ETF Factors</h2> <p>To better understand the economic drivers behind the principal components identified through PCA and Sparse PCA, I conducted a correlation analysis with major ETF factors. This analysis helps to link the statistical components derived from my stock return data to broader market trends and investment styles, particularly in the context of the ongoing bull market, recent Federal Reserve actions, and global geopolitical events.</p> <h3 id="methodology">Methodology</h3> <ol> <li> <p><strong>Data Preparation</strong>: I used the PCA and Sparse PCA component scores calculated from my stock return data, along with the returns of selected ETFs representing various investment factors (e.g., momentum, value, growth).</p> </li> <li> <p><strong>Correlation Analysis</strong>: I computed the Pearson correlation coefficients between each principal component (both from standard PCA and Sparse PCA) and the ETF returns.</p> </li> <li> <p><strong>Visualization</strong>: The correlations were visualized using heatmaps to provide an intuitive understanding of the relationships between components and ETF factors.</p> </li> <li> <p><strong>Lag Analysis</strong>: To identify potential leading or lagging relationships, I performed a lag analysis, calculating correlations at different time shifts between the components and ETF factors.</p> </li> </ol> <h3 id="results">Results</h3> <h4 id="correlation-heatmaps">Correlation Heatmaps</h4> <div class="l-body-outset"> <iframe src="/assets/plotly/RegimeShift/pca_etf_correlation_heatmap.html" frameborder="0" scrolling="no" height="600px" width="100%" style="border: 1px dashed grey;"></iframe> </div> <div class="caption"> Heatmap of correlations between PCA components and ETF factors. Plot credit: <a href="https://github.com/parvanovkp/RegimeShift/blob/main/interpretation_results/pca_etf_correlation_heatmap.html">Kal Parvanov/GitHub</a> </div> <div class="l-body-outset"> <iframe src="/assets/plotly/RegimeShift/sparse_pca_etf_correlation_heatmap.html" frameborder="0" scrolling="no" height="600px" width="100%" style="border: 1px dashed grey;"></iframe> </div> <div class="caption"> Heatmap of correlations between Sparse PCA components and ETF factors. Plot credit: <a href="https://github.com/parvanovkp/RegimeShift/blob/main/interpretation_results/sparse_pca_etf_correlation_heatmap.html">Kal Parvanov/GitHub</a> </div> <h4 id="lag-analysis">Lag Analysis</h4> <p>I conducted a lag analysis to identify potential leading or lagging relationships between the PCA components and ETF factors. However, no significant lead-lag relationships were found. The most substantial lagged correlations were around 0.05, compared to non-lagged correlations of up to 0.90. For readers interested in the detailed results, the lag analysis plots are available <a href="https://github.com/parvanovkp/RegimeShift/tree/main/interpretation_results">here</a>.</p> <h3 id="interpretation">Interpretation</h3> <p>The correlation heatmaps reveal several interesting patterns and relationships between the PCA components and ETF factors. These should be interpreted in the context of the ongoing bull market since 2022, recent Federal Reserve actions, and significant global events:</p> <ol> <li> <p><strong>Full PCA Results:</strong></p> <table> <thead> <tr> <th>PC</th> <th>Strong Positive Correlations</th> <th>Strong Negative Correlations</th> </tr> </thead> <tbody> <tr> <td>PC1</td> <td>XLI (0.929), DIA (0.917), XLB (0.905), XLF (0.856), IWM (0.850)</td> <td>VXX (-0.608)</td> </tr> </tbody> </table> <ul> <li>PC1 captures broad market movements across various sectors and cap sizes, reflecting the overall bullish trend despite global uncertainties.</li> <li>The strong positive correlations with industrial (XLI) and materials (XLB) ETFs may reflect increased defense spending and the impact of sanctions on Russia, a major exporter of basic materials.</li> <li>The positive correlation with financials (XLF) aligns with the high interest rate environment up to August 2024.</li> <li>The negative correlation with VXX indicates lower market volatility, typical of a bull market, but also suggesting that geopolitical risks might be underpriced.</li> <li>The remaining principal components show weaker correlations with the ETFs, potentially due to noise or more nuanced market dynamics.</li> </ul> </li> <li> <p><strong>Sparse PCA Results:</strong></p> <table> <thead> <tr> <th>PC</th> <th>Strong Positive Correlations</th> <th>Strong Negative Correlations</th> </tr> </thead> <tbody> <tr> <td>PC1</td> <td>XLI (0.946), XLB (0.879), DIA (0.874), XLF (0.819), IWM (0.806)</td> <td>VXX (-0.590)</td> </tr> <tr> <td>PC2</td> <td>XLU (0.909)</td> <td>XLK (-0.158), VXX (-0.131)</td> </tr> <tr> <td>PC3</td> <td>XLE (0.944), XLB (0.823)</td> <td>VXX (-0.416)</td> </tr> <tr> <td>PC4</td> <td>VXX (0.684)</td> <td>SPY (-0.862), DIA (-0.835), XLC (-0.816), XLI (-0.807), XLY (-0.807)</td> </tr> <tr> <td>PC5</td> <td>XLRE (0.917), DIA (0.731), XLI (0.727), XLF (0.722)</td> <td>VXX (-0.444)</td> </tr> <tr> <td>PC6</td> <td>XLI (0.890), XLB (0.884), IWM (0.882), DIA (0.868)</td> <td>VXX (-0.570)</td> </tr> <tr> <td>PC7</td> <td>XLF (0.942), DIA (0.883), XLI (0.868), XLB (0.843)</td> <td>VXX (-0.525)</td> </tr> <tr> <td>PC8</td> <td>XLV (0.827), XLP (0.824), DIA (0.757), XLF (0.754)</td> <td>VXX (-0.338)</td> </tr> <tr> <td>PC9</td> <td>QQQ (0.938), XLK (0.918), SPY (0.910)</td> <td>VXX (-0.728)</td> </tr> <tr> <td>PC10</td> <td>IWM (0.590), DIA (0.588), XLV (0.577), XLI (0.562)</td> <td>VXX (-0.406)</td> </tr> </tbody> </table> <ul> <li>PC1 in Sparse PCA mirrors the Full PCA results, confirming its role in capturing the broad bull market trend amidst global tensions.</li> <li>PC2’s strong correlation with utilities (XLU) and negative correlation with technology (XLK) might reflect a defensive positioning due to geopolitical uncertainties and anticipation of interest rate changes.</li> <li>PC3’s strong correlation with energy (XLE) and materials (XLB) likely captures the impact of the Ukraine war, sanctions on Russia, and the evolving BRICS dynamics on commodity markets.</li> <li>PC4’s relationship with volatility (VXX) and negative correlations with broad market ETFs might reflect market reactions to both Fed policy changes and escalating conflicts in the Middle East.</li> </ul> </li> </ol> <p>These results expand upon the sector-based analysis findings, now contextualized within the current economic and geopolitical environment:</p> <ul> <li>The strong performance of industrials and materials aligns with increased defense spending and global supply chain realignments.</li> <li>The varied correlations across different sectors reflect complex market dynamics as the economy navigates high interest rates, geopolitical tensions, and potential de-dollarization efforts by BRICS nations.</li> <li>The consistent negative correlation with volatility across several PCs is intriguing, given the numerous global risk factors, and may suggest a degree of complacency in the market.</li> </ul> <h3 id="implications">Implications</h3> <ol> <li> <p><strong>Market Dynamics</strong>: The strong correlations between the first few PCs and broad market ETFs confirm the resilience of the bull market despite significant global challenges. However, the distinct sector correlations in later PCs suggest nuanced market responses to changing economic conditions, monetary policy, and geopolitical events.</p> </li> <li> <p><strong>Sector Rotation</strong>: The varying correlations across PCs, particularly in Sparse PCA, highlight potential sector rotation strategies as the market adapts to the evolving interest rate environment and global tensions. The strong performance of utilities in PC2, for instance, might indicate a shift towards defensive sectors in anticipation of economic or geopolitical shocks.</p> </li> <li> <p><strong>Risk Management</strong>: While the negative correlations with VXX suggest overall low volatility, the presence of a volatility-correlated component (PC4 in Sparse PCA) indicates the importance of monitoring potential market stress, especially given the complex global situation. The apparent low volatility despite numerous risk factors warrants careful consideration in risk management strategies.</p> </li> <li> <p><strong>Factor Investing</strong>: The results suggest that PCA and Sparse PCA can effectively identify underlying factors in the market that respond to both broad economic trends and specific geopolitical events. This could inform more dynamic factor-based investment strategies that adapt to rapidly changing global conditions.</p> </li> <li> <p><strong>Economic Indicators</strong>: The strong correlations with sector-specific ETFs provide insights into how different sectors respond to economic cycles, policy changes, and global events. For example, the energy and materials correlations in PC3 could be particularly informative about the impact of geopolitical tensions and potential shifts in global trade dynamics.</p> </li> </ol> <h3 id="conclusion-and-transition-to-dynamic-analysis">Conclusion and Transition to Dynamic Analysis</h3> <p>Through this correlation analysis, I’ve bridged the gap between statistical components derived from stock returns and broader market factors represented by ETFs, set against the backdrop of a prolonged bull market, significant monetary policy actions, and major geopolitical events.</p> <p>My analysis has revealed several key insights:</p> <ol> <li>The first principal component in both Full and Sparse PCA captures the broad bull market trend, with strong correlations to industrial, large-cap, and materials ETFs, reflecting both economic expansion and geopolitical influences.</li> <li>Subsequent components, particularly in Sparse PCA, reflect nuanced market dynamics, including potential sector rotations, responses to interest rate changes, and reactions to global events such as the Ukraine war and Middle East conflicts.</li> <li>The generally negative correlation of PCs with the volatility ETF (VXX) confirms overall market stability, which is surprising given the numerous global risk factors and warrants further investigation.</li> </ol> <p>These findings offer valuable insights for both academic research and practical investment applications, particularly in understanding market behavior during periods of economic expansion, monetary policy shifts, and significant geopolitical tensions.</p> <p>However, my analysis so far has been static, looking at the entire period from August 8 to September 19, 2024, as a whole. Given the significant events during this timeframe, including the Fed’s rate cut and escalating global tensions, a dynamic analysis becomes crucial. To capture these evolving patterns and potential regime shifts, I will next employ a Rolling Window PCA approach.</p> <p>In the following section, I will execute rolling PCA with a 10-day window to observe how these relationships change throughout my study period. This dynamic analysis will allow me to track changes in explained variance and loadings over time, potentially revealing how the market adapted to the sudden rate cut on September 18, 2024, and reacted to ongoing geopolitical developments. By applying this technique to both standard PCA and Sparse PCA, I aim to gain a more nuanced understanding of how market dynamics evolved in response to these significant economic and political events, and how different sectors and factors contributed to these changes over shorter time frames.</p> <h2 id="rolling-window-pca">Rolling Window PCA</h2> <p>While the full-period PCA and Sparse PCA analyses provide valuable insights into the overall market structure during the study period, they don’t capture the dynamic nature of financial markets. To address this limitation and identify potential regime shifts or evolving market dynamics, I implemented a Rolling Window PCA approach. This method allows for a more granular examination of how principal components and their relationships with market sectors change over time.</p> <h3 id="methodology-1">Methodology</h3> <p>I applied both standard PCA and Sparse PCA using a rolling window approach with the following parameters:</p> <ol> <li><strong>Window Size</strong>: 10 trading days</li> <li><strong>Step Size</strong>: 1 trading day</li> <li><strong>Number of Components</strong>: 10 (consistent with the full-period analysis)</li> </ol> <p>For each window, I performed the following steps:</p> <ol> <li>Extracted the data for the current 10-day window.</li> <li>Applied standard PCA and Sparse PCA to this subset of data.</li> <li>Calculated sector loadings for each principal component.</li> <li>Computed market stability and deviation from the full-period analysis.</li> </ol> <h3 id="stability-and-deviation-metrics">Stability and Deviation Metrics</h3> <p>To quantify the changes in market structure over time, I introduced two key metrics:</p> <ol> <li> <p><strong>Market Stability</strong>: This metric measures the similarity between consecutive time windows. I calculated it using the cosine similarity between the sector loadings of adjacent windows. A higher value indicates greater stability (less change) between windows, while a lower value suggests more significant shifts in market structure.</p> <p>Mathematically, for two consecutive windows $i$ and $i+1$, the stability is calculated as:</p> \[\mathrm{Stability}_{i, i+1} = 1 - \mathrm{cosine\_distance}(\mathrm{loadings}_i, \mathrm{loadings}_{i+1})\] <p>where $\mathrm{loadings}$ are the flattened arrays of sector loadings across all components.</p> </li> <li> <p><strong>Deviation from Full Period</strong>: This metric quantifies how much each rolling window’s market structure deviates from the full-period analysis. It’s calculated as the cosine similarity between the sector loadings of each rolling window and the full-period sector loadings. A higher value indicates that the window’s market structure is more similar to the full-period structure, while a lower value suggests a greater deviation.</p> <p>For each window $i$, the deviation is calculated as:</p> \[\mathrm{Deviation}_i = 1 - \mathrm{cosine\_distance}(\mathrm{loadings}_i, \mathrm{loadings}_{\mathrm{full}})\] <p>where $\mathrm{loadings}_{\mathrm{full}}$ are the sector loadings from the full-period analysis.</p> </li> </ol> <p>These metrics provide insights into the evolving nature of market dynamics and help identify periods of significant change or stability.</p> <h3 id="interactive-dashboard">Interactive Dashboard</h3> <p>To visualize the results of the rolling window analysis comprehensively, I created an interactive dashboard using Plotly. This dashboard includes:</p> <ol> <li>Full PCA Sector Loadings Heatmap</li> <li>Sparse PCA Sector Loadings Heatmap</li> <li>Market Stability Plot</li> <li>Full Period Deviation Plot</li> </ol> <p>The dashboard features a slider that allows for easy navigation through different time windows, providing a dynamic view of how market structure evolves over the study period.</p> <div class="l-page"> <iframe src="/assets/plotly/RegimeShift/interactive_dashboard.html" frameborder="0" scrolling="no" height="700px" width="100%" style="border: 1px dashed grey;"></iframe> </div> <div class="caption"> Interactive Dashboard for Rolling Window PCA Analysis. The slider at the bottom allows navigation through different time windows. Plot credit: <a href="https://github.com/parvanovkp/RegimeShift/blob/main/rolling_window_analysis/interactive_dashboard.html">Kal Parvanov/GitHub</a> </div> <h3 id="results-and-interpretation">Results and Interpretation</h3> <h4 id="rolling-window-pca-and-sparse-pca">Rolling Window PCA and Sparse PCA</h4> <p>The rolling window analysis revealed several interesting patterns and shifts in market dynamics over the study period:</p> <ol> <li> <p><strong>Sector Dominance Shifts</strong>: In the early windows (August 23 to August 28), Financial Services consistently appeared as a dominant sector across multiple principal components in both Full PCA and Sparse PCA. However, this dominance waned in later windows, giving way to a more diverse representation of sectors.</p> </li> <li> <p><strong>Energy Sector Prominence</strong>: The Energy sector showed persistent importance, particularly in PC2 of both Full PCA and Sparse PCA, from September 4 onwards. This could be related to ongoing geopolitical tensions and their impact on global energy markets.</p> </li> <li> <p><strong>Industrials Sector Consistency</strong>: The Industrials sector maintained a consistent presence, especially in PC3 of both PCA methods, suggesting its steady influence on market movements throughout the period.</p> </li> <li> <p><strong>Emergence of Consumer Sector</strong>: Towards the latter half of the study period (from September 11 onwards), the Consumer sector began to feature more prominently, particularly in Sparse PCA results. This could indicate shifting market focus towards consumer behavior and spending patterns.</p> </li> <li> <p><strong>Utilities and Real Estate Fluctuations</strong>: These sectors showed intermittent significance, potentially reflecting changing investor sentiments about defensive stocks and interest rate expectations.</p> </li> </ol> <h4 id="market-stability-analysis">Market Stability Analysis</h4> <p>The Market Stability plot revealed several key insights:</p> <ol> <li> <p><strong>Overall Trend</strong>: The market showed relatively high stability throughout the period, with most values above 0.8, indicating consistent market structures.</p> </li> <li> <p><strong>Periods of Instability</strong>: Notable drops in stability occurred on August 30 (0.771), September 12 (0.771), and September 19 (0.763). These dates coincide with significant market events:</p> <ul> <li>August 30: Likely market reaction to the approaching month-end and anticipation of September economic data.</li> <li>September 12: Corresponded with the release of producer-price inflation data, which influenced market expectations about the Federal Reserve’s actions <a href="https://www.wsj.com/livecoverage/stock-market-today-dow-sp500-nasdaq-live-09-12-2024">WSJ, Sept. 12, 2024</a>.</li> <li>September 19: Immediate market response to the Federal Reserve’s unexpected 50 basis point rate cut on September 18 <a href="https://www.wsj.com/livecoverage/fed-interest-rate-cut-inflation-live-09-18-2024">WSJ, Sept. 18, 2024</a>.</li> </ul> </li> <li> <p><strong>Periods of High Stability</strong>: The market showed peak stability on September 6 (0.972) and September 16 (0.972), suggesting periods of market consensus or reduced uncertainty.</p> </li> </ol> <h4 id="deviation-from-full-period-analysis">Deviation from Full Period Analysis</h4> <p>The Deviation from Full Period plot provided additional insights:</p> <ol> <li> <p><strong>Overall Trend</strong>: The deviation generally increased over time, indicating that market structures in later windows differed more from the full-period analysis than earlier windows.</p> </li> <li> <p><strong>Significant Deviations</strong>: The largest deviations occurred on September 13 (0.545), September 16 (0.541), and September 17 (0.593). These dates align with increased market speculation about the Federal Reserve’s upcoming rate decision <a href="https://www.wsj.com/livecoverage/stock-market-today-dow-sp500-nasdaq-live-09-13-2024">WSJ, Sept. 13, 2024</a>.</p> </li> <li> <p><strong>Period of Least Deviation</strong>: The market structure was most similar to the full-period analysis on September 3 (0.847), possibly indicating a “typical” market state during this time.</p> </li> </ol> <h3 id="implications-and-insights">Implications and Insights</h3> <p>The rolling window approach provided valuable insights that were not apparent in the full-period analysis:</p> <ol> <li> <p><strong>Dynamic Sector Influences</strong>: The analysis revealed how different sectors’ influences on market movements evolved over time, reflecting changing economic conditions and investor sentiments.</p> </li> <li> <p><strong>Sensitivity to Economic Events</strong>: The stability and deviation metrics effectively captured market reactions to key economic events and policy decisions, particularly around the Federal Reserve’s rate cut.</p> </li> <li> <p><strong>Market Adaptation</strong>: The increasing deviation from the full-period analysis over time suggests that market structures were adapting to new information and changing economic landscapes.</p> </li> <li> <p><strong>Trading Strategy Implications</strong>: The identification of periods with distinct market structures could inform the development of adaptive trading strategies that adjust to changing market dynamics.</p> </li> <li> <p><strong>Risk Management</strong>: Periods of low stability or high deviation from the full-period analysis might warrant increased caution in risk management practices.</p> </li> </ol> <p>This rolling window analysis has provided a nuanced view of market dynamics, capturing short-term shifts and reactions to economic events that were not visible in the full-period analysis. However, it’s important to note that this approach is sensitive to window size selection and may not capture very short-term fluctuations.</p> <p>In the next section I will examine intraday patterns, where I will attempt to explore how these broader market dynamics manifest within the trading day, potentially revealing additional layers of market behavior and investor decision-making processes.</p> <h2 id="intraday-pattern-analysis">Intraday Pattern Analysis</h2> <p>While the previous analyses focused on daily and multi-day market dynamics, intraday patterns can reveal crucial short-term behaviors within a single trading day. These patterns are often influenced by factors such as market open and close effects, lunch hour trading lulls, and scheduled economic announcements. In this section, I delve into the intraday patterns of stock returns using Sparse PCA techniques to uncover how sector influences change throughout the trading day.</p> <h3 id="methodology-2">Methodology</h3> <p>To analyze intraday patterns, I implemented the following approach:</p> <ol> <li> <p><strong>Data Segmentation</strong>: The trading day was divided into 13 intraday periods, balancing granularity with statistical robustness. Each period $p$ is represented by a time interval:</p> \[p_i = [t_i, t_{i+1}), \quad i = 1, 2, ..., 13\] <p>where $t_1$ corresponds to market open (9:30 AM) and $t_{14}$ to market close (4:00 PM).</p> </li> <li> <p><strong>Sparse PCA Application</strong>: For each intraday period $p_i$, Sparse PCA was applied with 10 components:</p> \[X_{p_i} = US^T + E\] <p>where $X_{p_i}$ is the data matrix for period $p_i$, $U$ are the sparse loadings, $S$ are the components, and $E$ is the error term.</p> </li> <li> <p><strong>Sector Loading Calculation</strong>: For each component $j$ and sector $S$ in period $p_i$, the sector loading $SL_{S,j,p_i}$ was calculated as:</p> \[SL_{S,j,p_i} = \sum_{k \in S} \max(L_{k,j,p_i}, 0)\] <p>where $L_{k,j,p_i}$ is the loading of stock $k$ in component $j$ for period $p_i$. These were then normalized across sectors:</p> \[NSL_{S,j,p_i} = \frac{SL_{S,j,p_i}}{\sum_{S'} SL_{S',j,p_i}}\] </li> <li> <p><strong>Visualization</strong>: Two main visualizations were created:</p> <ul> <li>An intraday heatmap showing $NSL_{S,j,p_i}$ for all sectors, components, and periods.</li> <li> <p>A sector influence plot demonstrating the average sector influence:</p> \[ASI_{S,p_i} = \frac{\sum_{j=1}^{10} NSL_{S,j,p_i}}{\sum_{S'} \sum_{j=1}^{10} NSL_{S',j,p_i}}\] </li> </ul> </li> </ol> <p>This approach allows for a comprehensive examination of intraday market dynamics, potentially revealing patterns not visible in daily or weekly analyses.</p> <h3 id="results-and-interpretation-1">Results and Interpretation</h3> <p>The intraday pattern analysis yielded two key visualizations that provide insights into how sector influences change throughout the trading day.</p> <h4 id="intraday-patterns-heatmap">Intraday Patterns Heatmap</h4> <p>The first visualization is a heatmap showing the intraday patterns of sector influence on Sparse PCA components:</p> <div class="l-page"> <iframe src="/assets/plotly/RegimeShift/intraday_patterns_heatmap.html" frameborder="0" scrolling="yes" height="800px" width="100%" style="border: 1px dashed grey;"></iframe> </div> <div class="caption"> Heatmap of Intraday Patterns: Sector Influence on Sparse PCA Components. Plot credit: <a href="https://github.com/parvanovkp/RegimeShift/blob/main/interpretation_results/intraday_patterns_heatmap.html">Kal Parvanov/GitHub</a> </div> <p>The intraday patterns heatmap reveals several interesting trends in sector influence across different periods of the trading day:</p> <ol> <li> <p><strong>Sector Dominance Variability</strong>: Different sectors dominate various principal components (PCs) throughout the day. For instance, Financial Services dominates PC1 at market open (09:30-10:00) but shifts to PC7 by mid-morning (10:30-11:00).</p> </li> <li> <p><strong>Technology Sector Influence</strong>: Technology shows strong influence in early trading, particularly in PC2 (09:30-10:00) and PC3 (10:00-10:30), but its dominance becomes more sporadic as the day progresses.</p> </li> <li> <p><strong>Energy Sector Consistency</strong>: The Energy sector consistently influences PC3 throughout most of the day, suggesting a stable pattern in energy-related market movements.</p> </li> <li> <p><strong>Utilities Sector Pattern</strong>: Utilities frequently dominate PC5 or PC6, especially in the middle of the trading day, indicating a consistent role in explaining market variance during these periods.</p> </li> <li> <p><strong>Consumer Sector Variability</strong>: The Consumer sector shows high variability, dominating different PCs at different times, which might reflect changing consumer behavior or market sentiment throughout the day.</p> </li> <li> <p><strong>End-of-Day Shifts</strong>: In the final trading period (15:30-16:00), there’s a noticeable shift in sector influences, with Technology regaining prominence in PC2 and Financial Services becoming more influential in PC3 and PC8.</p> </li> </ol> <p>These patterns suggest that the market’s underlying structure, as captured by the principal components, evolves throughout the trading day, with different sectors playing varying roles in explaining market variance at different times.</p> <h4 id="sector-influence-changes">Sector Influence Changes</h4> <p>The second visualization is a line plot showing how sector influences change throughout the trading day:</p> <div class="l-page"> <iframe src="/assets/plotly/RegimeShift/sector_influence_changes.html" frameborder="0" scrolling="no" height="600px" width="100%" style="border: 1px dashed grey;"></iframe> </div> <div class="caption"> Changes in Sector Influence Throughout the Trading Day. Plot credit: <a href="https://github.com/parvanovkp/RegimeShift/blob/main/interpretation_results/sector_influence_changes.html">Kal Parvanov/GitHub</a> </div> <p>The sector influence changes plot provides a comprehensive view of how each sector’s overall influence evolves throughout the trading day:</p> <ol> <li> <p><strong>Consumer Sector Volatility</strong>: The Consumer sector shows the highest volatility in influence, with a notable spike (32.54%) during the 11:00-11:30 period, possibly reflecting increased trading activity around consumer-related news or data releases.</p> </li> <li> <p><strong>Technology Sector Trend</strong>: Technology starts the day with high influence (16.19% at open) but generally decreases throughout the day before rising again in the final period (15.37% at close).</p> </li> <li> <p><strong>Financial Services Fluctuations</strong>: Financial Services shows significant fluctuations, with peaks in the early afternoon (14.72% at 13:30-14:00) and late afternoon (20.39% at 15:00-15:30), possibly aligning with key economic announcements or end-of-day portfolio adjustments.</p> </li> <li> <p><strong>Energy Sector Stability</strong>: The Energy sector maintains a relatively stable influence throughout the day, ranging between 4.79% and 8.76%, suggesting consistent trading patterns.</p> </li> <li> <p><strong>Utilities Sector Pattern</strong>: Utilities show increased influence in the early afternoon, peaking at 18.15% during 14:00-14:30, which could be related to daily energy consumption patterns or specific market events.</p> </li> <li> <p><strong>Communication Services Spikes</strong>: Communication Services experiences notable spikes in influence, particularly during the 11:00-11:30 and 13:30-14:00 periods, possibly reflecting sector-specific news or trading patterns.</p> </li> </ol> <h3 id="key-intraday-patterns-observed">Key Intraday Patterns Observed</h3> <ol> <li> <p><strong>Opening and Closing Effects</strong>: Many sectors show distinct patterns at market open and close. For example, Technology and Consumer sectors have higher influence at the start and end of the trading day.</p> </li> <li> <p><strong>Midday Shifts</strong>: Several sectors experience significant changes in influence during the middle of the trading day, particularly around the 11:00-11:30 and 14:00-14:30 periods. This could be related to lunch hour effects or the timing of economic announcements.</p> </li> <li> <p><strong>Sector Rotations</strong>: There are observable “rotations” in sector influence throughout the day. As one sector’s influence decreases, another often increases, suggesting a dynamic reallocation of trading focus.</p> </li> <li> <p><strong>Persistent Components</strong>: Some sectors, like Energy and Utilities, show persistent influence on specific principal components throughout the day, indicating stable underlying factors in these sectors.</p> </li> <li> <p><strong>Afternoon Volatility</strong>: The period from 14:00 to market close shows increased volatility in sector influences, potentially reflecting heightened trading activity as market participants position themselves before the close.</p> </li> </ol> <p>These patterns provide valuable insights into the intraday dynamics of the stock market, revealing how different sectors drive market movements at various times of the day. However, it’s important to note that these patterns are based on averaged data over the study period and may not reflect day-to-day variations or responses to specific events.</p> <h3 id="implications-1">Implications</h3> <p>The intraday pattern analysis provides valuable insights into stock market behavior within a trading day. Key implications include:</p> <ol> <li> <p><strong>Trading Strategies</strong>:</p> <ul> <li>Energy sector’s consistent influence on PC3 could inform sector rotation strategies.</li> <li>Consumer sector’s high volatility, especially around 11:00-11:30, might offer short-term trading opportunities.</li> <li>Technology sector’s changing influence throughout the day could guide entry and exit points for tech-focused trades.</li> </ul> </li> <li> <p><strong>Risk Management</strong>:</p> <ul> <li>Increased afternoon volatility, particularly from 14:00 onwards, suggests a need for tighter risk controls during these periods.</li> <li>Stable sectors like Utilities could potentially hedge against more volatile sector movements.</li> </ul> </li> <li> <p><strong>Market Microstructure</strong>:</p> <ul> <li>Consistent patterns in certain sectors might indicate regular algorithmic trading activities.</li> <li>Spikes in sector influence could reflect high-frequency trading strategies’ impact.</li> </ul> </li> <li> <p><strong>Liquidity Provision</strong>:</p> <ul> <li>Afternoon volatility in sector influences might require liquidity providers to adjust spreads and inventory levels.</li> <li>Consistent influence of sectors like Energy could inform stable liquidity provision strategies for related stocks.</li> </ul> </li> <li> <p><strong>Portfolio Rebalancing</strong>:</p> <ul> <li>Observed sector rotations and end-of-day shifts could guide optimal times for portfolio rebalancing, potentially minimizing market impact.</li> </ul> </li> </ol> <h3 id="limitations-and-future-work">Limitations and Future Work</h3> <p>While this analysis provides valuable insights, it’s important to acknowledge its limitations:</p> <ol> <li> <p><strong>Time Period Specificity</strong>: The patterns observed are specific to the analyzed time period (August to September 2024) and may not generalize to other market conditions or time frames. Market dynamics can vary significantly across different periods due to changing economic conditions, geopolitical events, or structural changes in the market.</p> </li> <li> <p><strong>Granularity Trade-off</strong>: The choice of 13 intraday periods balances detail with statistical robustness, but different granularities might reveal additional patterns. The current segmentation might miss very short-term fluctuations or smooth over rapid changes in sector influences.</p> </li> <li> <p><strong>Market Events</strong>: The analysis doesn’t explicitly account for scheduled market events which could significantly impact intraday patterns. Major economic announcements, earnings releases, or other news events could drive some of the observed patterns.</p> </li> <li> <p><strong>Aggregation Effects</strong>: By aggregating data across multiple trading days, this analysis may obscure day-specific patterns or anomalies that could be significant for certain trading strategies.</p> </li> <li> <p><strong>Sector Classification</strong>: The sector classifications used in this analysis are based on standard categorizations, which may not always capture the nuanced relationships between companies or subsectors.</p> </li> </ol> <p>Future work could address these limitations and extend the analysis in several ways:</p> <ul> <li>Extending the analysis to cover longer time periods and different market regimes, allowing for the identification of more stable intraday patterns and their evolution over time.</li> <li>Incorporating external data sources, such as news sentiment, economic calendars, or order flow data, to contextualize intraday patterns and potentially explain anomalies.</li> <li>Conducting a more granular analysis of subsectors or individual stocks within sectors to identify potential leaders or laggards in intraday pattern formation.</li> <li>Developing and backtesting trading strategies based on the observed intraday patterns to assess their practical applicability and robustness.</li> <li>Analyzing the impact of different time zone markets (e.g., European or Asian markets) on the observed patterns in the U.S. market.</li> </ul> <p>This granular approach to market analysis has the potential to refine trading strategies, improve risk management practices, and deepen one’s understanding of the complex, ever-changing nature of financial markets. By addressing these limitations and pursuing these avenues for future research, one can continue to enhance one’s understanding of intraday market dynamics and their implications for various market participants.</p> <h2 id="conclusion-key-findings-and-future-research-directions">Conclusion: Key Findings and Future Research Directions</h2> <p>This comprehensive analysis of S&amp;P 500 stocks using PCA and Sparse PCA techniques has yielded several significant insights into market dynamics and sector behaviors:</p> <ol> <li>The full-period PCA revealed that capturing 80% of market variance required 154 components, highlighting the complexity of market dynamics.</li> <li>Sparse PCA effectively isolated key sector influences, with Technology, Consumer, and Financial Services sectors frequently emerging as dominant factors.</li> <li>The correlation analysis with ETF factors demonstrated strong relationships between principal components and sector-specific ETFs, providing a bridge between statistical findings and real-world market factors.</li> <li>Rolling window PCA uncovered evolving market structures, particularly around significant events like the Federal Reserve’s rate cut in September 2024.</li> <li>Intraday pattern analysis revealed distinct sector behaviors at different times of the trading day, such as the Consumer sector’s high volatility and the Energy sector’s consistent influence.</li> </ol> <p>These findings have several practical implications:</p> <ul> <li>For portfolio management, the identified sector influences and their temporal variations can inform more dynamic asset allocation strategies.</li> <li>Risk assessment models can be enhanced by incorporating the changing market structures revealed through rolling window analysis.</li> <li>Traders can leverage the intraday patterns to optimize entry and exit points for sector-specific trades.</li> </ul> <p>While this analysis provides valuable insights, there are several avenues for future research:</p> <ol> <li>Expanding the dataset to cover longer time periods and different market regimes to test the stability of observed patterns.</li> <li>Incorporating alternative data sources, such as news sentiment or order flow data, to provide additional context to market movements.</li> <li>Applying advanced machine learning techniques, such as deep learning models, to capture more complex, non-linear relationships in the data.</li> </ol> <p>By pursuing these research directions, it is possible to further refine the understanding of market dynamics, potentially leading to more robust investment strategies and risk management practices. This project contributes to the field of quantitative finance and demonstrates the efficacy of dimensionality reduction techniques in uncovering hidden structures within complex financial systems such as the stock market.</p> <h2 id="appendix">Appendix</h2> <p><strong>Project Code:</strong> <a href="https://github.com/parvanovkp/RegimeShift">Kal Parvanov/Github</a></p>]]></content><author><name>Kaloyan Parvanov</name></author><category term="machine learning"/><category term="finance"/><category term="data analysis"/><category term="pca"/><category term="sparse pca"/><category term="dimensionality reduction"/><category term="financial analysis"/><category term="regime shift"/><summary type="html"><![CDATA[This project explores PCA and Sparse PCA on 457 SP500 stocks, using 2-minute interval data over 31 trading days (August 8 to September 19, 2024). The focus is on experimenting with dimensionality reduction techniques to identify regime shifts and key factors driving stock returns.]]></summary></entry><entry><title type="html">The Damped Unforced Pendulum Problem</title><link href="https://www.kparvanov.com/blog/2023/distill/" rel="alternate" type="text/html" title="The Damped Unforced Pendulum Problem"/><published>2023-11-30T00:00:00+00:00</published><updated>2023-11-30T00:00:00+00:00</updated><id>https://www.kparvanov.com/blog/2023/distill</id><content type="html" xml:base="https://www.kparvanov.com/blog/2023/distill/"><![CDATA[<h2 id="introduction-understanding-the-damped-pendulum-problem">Introduction: Understanding the Damped Pendulum Problem</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/NN_Final_Project/foucault.jpg" sizes="95vw"/> <img src="/assets/img/NN_Final_Project/foucault.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Foucault's pendulum at the Pantheon in Paris, France. Photo credit: <a href="https://www.flickr.com/photos/msk13/4818027399/">Misko/Flickr</a> </div> <p>The pendulum problem is a fundamental problem in classical mechanics and as such is often one of the topics taught to first year physics students in college. The earliest well-known use of pendulums in modern history dates back to 1656 when the Dutch scientist Christiaan Huygens used a pendulum as a time-keeping device.<d-cite key="huygens-hist"></d-cite> This novel application of the pendulum was built upon the work of the 16th century Italian polymath Galileo Galilei, who discovered that the period<d-footnote>The time for each complete swing.</d-footnote> of the pendulum’s swing is independent of its amplitude<d-footnote>The length of the swing.</d-footnote>, and that for small enough swings it is also isochronic<d-footnote>Constant in time.</d-footnote>.<d-cite key="galileo"></d-cite> Later, in 1851, the French physicist Léon Foucault developed a pendulum experiment<d-footnote>A 28kg mass attached to a 68m long thin metal wire was suspended from the half-dome ceiling of the Pantheon in Paris and tracked over a long period of time to show that the pendulum's plane of oscillation rotates.</d-footnote> in order to demonstrate the rotation of the Earth.<d-cite key="foucault"></d-cite> From helping us measure time and navigate the seas to helping us prove that the Earth rotates, the pendulum and the study of the dynamical system that describes its motion have had a enormous effect on Physics and Engineering.</p> <p>The utility of the pendulum, however, extends far beyond its historical significance. Indeed, the pendulum problem is quite important in modern science and engineering. In fact, the underlying principles driving the behaviour of the pendulum are foundational in understanding the topics of harmonic motion and vibration control. These topics are particularly important in fields such as seismology and structural engineering. For example, pendulums are key components in the design of classical seismographs where they are used to measure and record the strength and duration of earth tremors and earthquakes. Such an application is pivotal in the field of seismic monitoring as it aids in the prediction of seismic events, thereby significantly improving public safety. Furthermore, structural engineers often employ pendulum principles in the design of systems and elements that would improve a building’s seismic resilience. More specifically, tuned mass dampers are used to mitigate the vibrations in structures caused by winds and earthquakes.<d-cite key="tuned_damper"></d-cite> If left unchecked these vibrations could otherwise lead to building collapse and a large number of fatalities. The simplicity of the pendulum, juxtaposed with its complex modern-day applications, clearly demonstrates how basic physical principles can be harnessed to solve contemporary engineering challenges. Nevertheless, for such complex solutions to be feasible one needs to rely on a detailed analytical framework to derive valuable insights about the underlying principles.</p> <style>.fixed-size-image{width:350px;height:250px;object-fit:cover}</style> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/NN_Final_Project/damper.jpg" sizes="95vw"/> <img src="/assets/img/NN_Final_Project/damper.jpg" class="img-fluid rounded z-depth-1 fixed-size-image" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> The tuned mass damper of Taipei 101 in Taipei City, Taiwan. Photo credit: <a href="https://www.flickr.com/photos/sk51/8342914631/in/photolist-dHeBRH-cYdM75-2jCaxyh-2hTxiPJ-2nWqcFr-9pXKsv-786x47-84fDFi-XwmqrQ-5bBakR-2mW8ZV1-7L5rCf-aALp8G-2mUxiF8-UJXh6T-bzo9qN-9iWdLQ-SrNtC8-FwwsFU-2oqdroL-dHeAk6-dHk3yj-dHk3ff-8gewme-dHeABP-4gUWrG-4W59q7-2hDjY8x-scYRu4-FUXrxo-r6tZZK-mDxm8X-5WK4MY-2nWqDNQ-2hUd5g8-pMNR8j-cxbmJy-2igoTqV-3ScBKH-cdadfE-8gew4R-2igoTBM-WRdWNJ-6ax2Es-5RofSj-6xg81v-2ijUJpf-fkY5NL-2iVKEw7-584KA">Paul Blair/Flickr</a> </div> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/NN_Final_Project/giphy_2.gif" sizes="95vw"/> <img src="/assets/img/NN_Final_Project/giphy_2.gif" class="img-fluid rounded z-depth-1 fixed-size-image" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> A tuned mass damper (TMD) in action. GIF credit: <a href="https://giphy.com/gifs/Dlubal-Software-mFN5aTa9OTcc2V2cgb">Dlubal/Giphy</a> </div> </div> </div> <p>The development of pendulum motion analysis within a dynamical systems framework has been a significant leap in the understanding of the pendulum problem in realistic<d-footnote>The physical experiments of old cannot satisfy the rigor and details demanded by modern-day engineers.</d-footnote> settings. Originating from the need to model more complex and real-world scenarios, the dynamical systems approach considers the effects of external stopping forces such as air-resistance and friction<d-footnote>Often referred to as damping forces.</d-footnote> on the motion of the pendulum. Unlike the idealized simple pendulum, which swings perpetually, the damped unforced pendulum experiences a gradual decrease in its oscillation amplitude due to resistive forces, ultimately coming to a rest. This more complex model allows for an accurate analysis of the behaviors of the pendulum, without the need to conduct complicated physical experiments. Furthermore, the concept of “damping” is crucial in understanding the various real-world oscillatory systems where energy dissipation is a common occurrence. One such example is the swinging of a building during an earthquake. Here the dynamical systems perspective not only offers a more accurate representation of pendulum motion, but also reveals intricate patterns and behaviors, like the transition from regular to chaotic behavior under certain conditions. This analytical framework, thus, unlocks deeper insights into the pendulum’s dynamics, showing its utility in designing systems that mimic or counteract particular motions, and allows engineers to predict complex behaviors without having to set up costly, and sometimes even impossible physical experiments.</p> <style>.pendulum{width:750px;height:420px;object-fit:cover}</style> <div class="row mt-3 l-gutter"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/NN_Final_Project/Pendulum-no-text.gif" sizes="95vw"/> <img src="/assets/img/NN_Final_Project/Pendulum-no-text.gif" class="img-fluid rounded pendulum" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption l-gutter" style="margin-top: -5px;"> An idealized pendulum in motion. GIF credit: <a href="https://en.m.wikipedia.org/wiki/File:Pendulum-no-text.gif">Stündle/Wikipedia</a> </div> <p>However, as the applications of pendulum principles grow increasingly complex, the corresponding dynamical systems modeling the desired behaviors become harder and harder to solve and analyze. Historically, this gave rise to the field of numerical analysis, which focused on developing various numerical schemes that are provably reliable in order to solve problems with no analytic<d-footnote>Closed-form.</d-footnote> solutions. For instance, in the realm of high-rise building design, where pendulum-like tuned mass dampers are employed to counteract wind and seismic forces, the intricate interplay of forces demands highly precise numerical solvers, which could often take days or even weeks to run. Similarly, in the field of aerospace engineering, the stabilization systems of spacecraft, which draw parallels to pendulum dynamics, require models capable of handling intricate, multi-dimensional forces.<d-cite key="navabi"></d-cite> Traditional numerical approaches, while effective for simpler systems, struggle with the computational load and precision required for these advanced applications. The need for faster, more efficient, and accurate modeling, thus, becomes apparent, hinting at the emergence of innovative approaches that can bridge these gaps.<d-cite key="lu2021"></d-cite> These evolving methods promise to offer deeper insights and lower computational costs, laying the groundwork for a new era of computational analysis in dynamics.</p> <p>As we stand on the brink of a new frontier in dynamical systems analysis, it is clear that our journey with the pendulum, from a simple timekeeping device to a complex model in engineering and physics has been nothing short of remarkable. The pendulum’s story teaches us the value of revisiting basic concepts with a fresh perspective, continually pushing the boundaries of our understanding and capabilities. In this era of rapid technological advancement, our quest to model and predict complex dynamical systems is more relevant than ever. It paves the way for innovative solutions that are not just faster and more precise, but also more accessible to a wider range of applications. This evolution in approach, subtly hinted at, but not yet fully explored, promises to revolutionize how we perceive and interact with the dynamic world around us. It faithfully embodies the very spirit of continuous discovery and improvement that drives the scientific effort.</p> <hr/> <h2 id="data-foundations-pendulum-dynamics-and-parameters">Data Foundations: Pendulum Dynamics and Parameters</h2> <p>This project is designed within the analysis framework of dynamical systems theory which attempts to model real-world time dependent systems referred to as ‘dynamical systems’. In particular the specifc model for the pendulum problem was chosen with a fine balance between realistic behavior and model complexity in mind. The two sub-sections below introduce the theoretical framework for modeling pendulum motion and explain the data generation process used to collect the data needed for training the Vanilla neural network model introduced in the next section.</p> <h3 id="the-theoretical-framework">The Theoretical Framework</h3> <p>The traditional 1D damped unforced pedulum problem is modeled by the second order homogenous constant coefficient non-linear oridinary differential equation (ODE):</p> \[\ddot{\theta}(t)+b \dot{\theta}(t)+\frac{g}{L} \sin (\theta(t))=0\] <p>where</p> <ul> <li>\(\ddot{\theta}(t)\) is the second derivative of \(\theta(t)\) with respect to time, representing the angular acceleration of the pendulum (\(rad/s^2\)).</li> <li>\(\dot{\theta}(t)\) is the first derivative of \(\theta(t)\) with respect to time, representing the angular velocity of the pendulum (\(rad/s\)).</li> <li>\(b\) is the damping coefficient, representing the effect of air resistance or other damping forces.</li> <li>\(g\) is the acceleration due to gravity.</li> <li>\(L\) is the length of the pendulum.</li> </ul> <p>Since the equation above is non-linear in nature, it does not have a closed-form solution. This, in turn, necessitates the use of numerical methods such as the those from the Runge-Kutta, Adams-Bashford and Adams-Moulton families of methods. It is important to note that throughout this article the notation,</p> \[\omega(t) = \dot{\theta}(t)\] <p>will be used for convenience and in accordance with the classical mechanics nomenclature.</p> <h3 id="data-generation">Data Generation</h3> <div class="row mt-3 l-page"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/NN_Final_Project/pendulum_motion.gif" sizes="95vw"/> <img src="/assets/img/NN_Final_Project/pendulum_motion.gif" class="img-fluid rounded" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption l-page"> A dynamic visualization of the IVP presented below. GIF credit: <a href="https://github.com/parvanovkp/Neural_Networks_Final_Project/blob/main/Pendulum%20Project/pendulum_motion.gif">Kal Parvanov/GitHub</a> </div> <p>An initial value problem (IVP) with parameters $b=0.5$, $g=9.81$ and $L=6$, and initial conditions $\theta(0)=\frac{\pi}{4}$ and $\omega(0)=0$ is used to generate data for the pendulum problem. The particular form of the IVP is written as</p> \[\begin{aligned} \ddot{\theta}(t)+0.5 \dot{\theta}(t)+\frac{9.81}{6} \sin (\theta(t)) &amp; =0, \\ \theta(0) &amp; =\frac{\pi}{4}, \\ \dot{\theta}(0) &amp; =0 . \end{aligned}\] <p>This form of the IVP, however, is not the standard form accepted by numerical solvers, which require a first order form. To convert the IVP to a first order form it is necessary to express it as a system of two first order ODEs with a corresponding vector of initial conditions. This can be done, by letting</p> \[\begin{aligned} \dot{\theta}(t) &amp; =\omega(t) \\ \dot{\omega}(t) &amp; =-0.5 \omega(t)-\frac{9.81}{6} \sin (\theta(t)) \end{aligned}\] <p>which then results in the linear system of first order ODEs with an initial value vector</p> \[\left[\begin{array}{c} \dot{\theta}(t) \\ \dot{\omega}(t) \end{array}\right]=\left[\begin{array}{c} \omega(t) \\ -0.5 \omega(t)-\frac{9.81}{6} \sin (\theta(t)) \end{array}\right], \quad\left[\begin{array}{c} \theta(0) \\ \omega(0) \end{array}\right]=\left[\begin{array}{l} \frac{\pi}{4} \\ 0 \end{array}\right]\] <p>The specific numerical solver used to solve the IVP above and generate the training data is the <code class="language-plaintext highlighter-rouge">solve_ivp</code> function from the Python <code class="language-plaintext highlighter-rouge">scipy</code> library. The underlying numerical method employed is the Runge-Kutta order 5 method, which is appropriate in this situation since the underlying ODE is not stiff. Below is the short code used to generate the training data for the Vanilla Neural Network.<d-footnote> Note that the Physics-Informed Neural Network does not need any training data since its training is entirely guided by its physics-informed loss function.</d-footnote> The code is used to generate 1000 solution points \(\theta_i\) and \(\omega_i\) <d-footnote>$1 \leq i \leq 1000$</d-footnote> at 1000 equispaced time points on the interval \([0,10]\).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">scipy.integrate</span> <span class="kn">import</span> <span class="n">solve_ivp</span>

<span class="c1"># Constants
</span><span class="n">b</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="c1"># damping coefficient
</span><span class="n">g</span> <span class="o">=</span> <span class="mf">9.81</span> <span class="c1"># acceleration due to gravity
</span><span class="n">L</span> <span class="o">=</span> <span class="mi">6</span> <span class="c1"># length of the pendulum
</span><span class="n">y0</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="c1"># Initial conditions
</span>
<span class="c1"># Differential Equation
</span><span class="k">def</span> <span class="nf">damped_pendulum</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">theta</span><span class="p">,</span> <span class="n">omega</span> <span class="o">=</span> <span class="n">y</span>
    <span class="n">dtheta_dt</span> <span class="o">=</span> <span class="n">omega</span>
    <span class="n">domega_dt</span> <span class="o">=</span> <span class="o">-</span><span class="n">b</span> <span class="o">*</span> <span class="n">omega</span> <span class="o">-</span> <span class="p">(</span><span class="n">g</span> <span class="o">/</span> <span class="n">L</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">dtheta_dt</span><span class="p">,</span> <span class="n">domega_dt</span><span class="p">]</span>

<span class="c1"># Generate and solve ODE
</span><span class="k">def</span> <span class="nf">generate_ode_solution</span><span class="p">(</span><span class="n">t_span</span><span class="p">,</span> <span class="n">y0</span><span class="p">,</span> <span class="n">t_eval</span><span class="p">):</span>
    <span class="k">return</span> <span class="nf">solve_ivp</span><span class="p">(</span><span class="n">damped_pendulum</span><span class="p">,</span> <span class="n">t_span</span><span class="p">,</span> <span class="n">y0</span><span class="p">,</span> <span class="n">t_eval</span><span class="o">=</span><span class="n">t_eval</span><span class="p">)</span>
</code></pre></div></div> <p>Finally, note that due to the nature of the ODE problem, there is no need for specific training, validation and test datasets. Both of the models presented in the next section will learn the IVP solution, namely \(\theta(t)\) and \(\omega(t)\), on the pre-specified domain<d-footnote>$[0,10]$</d-footnote> with an ability to predict \(\theta(t)\) and \(\omega(t)\) for any point \(t\) on that domain to a degree of accuracy comparable to that of numerical solvers. This is, in fact, due to Neural Networks’ ability to approximate functions, which is exactly what the solution to the IVP is. <d-cite key="HORNIK1991251"></d-cite></p> <hr/> <h2 id="modeling-pendulum-motion-neural-network-approaches">Modeling Pendulum Motion: Neural Network Approaches</h2> <p>This section presents two different Neural Network models that are trained to predict the values of the IVP presented in <a href="#data-generation">Data Generation</a>. The first model is a Vanilla Neural Network that uses the true \(\theta\) and \(\omega\) data obtained via the numerical solver <code class="language-plaintext highlighter-rouge">solve_ivp</code> and outputs \(\widehat{\theta}\) and \(\widehat{\omega}\). The reason for choosing the VNN model is for its function approximation properties and to have it serve as a control model for the more complicated Physics-Informed model.<d-cite key="HORNIK1991251"></d-cite> Physics-Informed Neural Networks, more commonly known as PINNs, have garnered quite a bit of interest in the scientific community since their original introduction in 2019.<d-cite key="RAISSI2019686"></d-cite> PINNs have the unique ability to incorporate the underlying physics of a problem into the Neural Network model and, as a result of this property, a PINN doesn’t need any training data. Its loss function is defined in such a way so that the physics principles of the problem define the penalty for wrong predictions. PINNs are, thus, quite promising tools for the solution of complicated differential equations where regular solvers might not work or take too long to produce results.</p> <h3 id="the-vanilla-neural-network">The Vanilla Neural Network</h3> <p>A simple neural network model, referred to as a “Vanilla Neural Network”, was selected as the control model to compare with the physics-informed neural network introduced in the next section. The VNN model consists of a simple feed-forward neural network with 1 input node for the \(t\)-values and 3 hidden layers with 32 units each, activated with a hyperbolic tangent function \(\tanh\). There are two output nodes, one for \(\widehat{\theta}\) and another for \(\widehat{\omega}\). The Loss function is custom-defined to use the sum of the \(\operatorname{MSE}\) (mean squared error) for \(\widehat{\theta}\) and \(\widehat{\omega}\) and is given by</p> \[\begin{align*} \operatorname{Loss}_{\mathrm{TOTAL}} &amp;= \operatorname{MSE}(\widehat{\theta}, \theta) + \operatorname{MSE}(\widehat{\omega}, \omega) \\ &amp;= \frac{1}{n} \sum^{n}_{i=1}(\widehat{\theta}_i - \theta_i)^2 + \frac{1}{n} \sum^{n}_{i=1}(\widehat{\omega}_i - \omega_i)^2 \\ &amp;= \frac{1}{n} \sum^{n}_{i=1}(\widehat{\theta}_i - \theta_i)^2 + (\widehat{\omega}_i - \omega_i)^2 \end{align*}\] <p>Outside of the custom defined mean-squared loss function, which is otherwise indifferent to the standard \(\operatorname{MSE}\) loss function in <code class="language-plaintext highlighter-rouge">keras</code>, the neural network architecture is identical to that of the standard FFNNs. Below is a diagram of the structure that was described above.</p> <div class="row mt-3 l-page"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/NN_Final_Project/Vanilla_NN_architecture_transparent.png" sizes="95vw"/> <img src="/assets/img/NN_Final_Project/Vanilla_NN_architecture_transparent.png" class="img-fluid rounded" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Vanilla Neural Net Architecture. Diagram credit: <a href="https://github.com/parvanovkp/Neural_Networks_Final_Project/blob/main/Vanilla_NN_architecture_transparent.png">Kal Parvanov/GitHub</a> </div> <p>The particular implementation in Python of the model can be seen below. For the full details, please refer to the project code link in the <a href="#appendix">Appendix</a>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="c1"># Neural Network Model
</span><span class="k">class</span> <span class="nc">DampedPendulumModel</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layers_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">tanh</span><span class="sh">'</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">output_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">layers_list</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="nf">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">output_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Vanilla Loss Function
</span><span class="k">def</span> <span class="nf">compute_vanilla_loss</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">actual_theta</span><span class="p">,</span> <span class="n">actual_omega</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">square</span><span class="p">(</span><span class="n">predictions</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">actual_theta</span><span class="p">)</span>

                          <span class="o">+</span> <span class="n">tf</span><span class="p">.</span><span class="nf">square</span><span class="p">(</span><span class="n">predictions</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">actual_omega</span><span class="p">))</span>

<span class="c1"># Vanilla NN Training Function
</span><span class="k">def</span> <span class="nf">train_vanilla</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">t_data</span><span class="p">,</span> <span class="n">theta_data</span><span class="p">,</span> <span class="n">omega_data</span><span class="p">,</span> <span class="n">epochs</span><span class="p">):</span>
    <span class="n">train_loss_record</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">plot_filenames</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nc">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
            <span class="n">predictions</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">constant</span><span class="p">(</span><span class="n">t_data</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">))</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="nf">compute_vanilla_loss</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">theta_data</span><span class="p">,</span> <span class="n">omega_data</span><span class="p">)</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="p">.</span><span class="nf">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">trainable_variables</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">apply_gradients</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">trainable_variables</span><span class="p">))</span>
        <span class="n">train_loss_record</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">loss</span><span class="p">.</span><span class="nf">numpy</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">train_loss_record</span><span class="p">,</span> <span class="n">plot_filenames</span>
</code></pre></div></div> <p>Note that the <code class="language-plaintext highlighter-rouge">GradientTape</code> method in <code class="language-plaintext highlighter-rouge">tensorflow</code> is used to record operations that would later be differentiated with the <code class="language-plaintext highlighter-rouge">gradient</code> method. This is TensorFlow’s approach to allowing the implementation of custom loss functions by users in Python.</p> <h3 id="the-physics-informed-neural-network">The Physics-Informed Neural Network</h3> <p>Physics-Informed Neural Networks (PINNs), despite their novelty, are well known for their useful properties in the study and solution of differential equations.<d-cite key="RAISSI2019686"></d-cite> By design, PINNs are guided by their physics-informed loss functions in the learning process, which makes them immune to all the issues related to having to train them on data, especially real-world data. Another aspect of PINNs, which is shared with Neural Networks in general, is that of the stark train-predict computing effort split. In other words, a researcher or an organization, might invest upfront into training a PINN to solve a complicated differential equation that would otherwise take days to solve with a regular numerical solver, in order to gain the benefit of almost instantaneous prediction. Such applications would be invaluable for predicting real-time hypersonic rocket positions, meteorological forecasts or the behaviors of financial derivatives, which are modelled by PDEs.</p> <p>The particular PINN that was chosen to learn and predict the behavior of the pendulum IVP, consists of a feed-forward neural network with the same specifications as the one presented in the previous sub-section, and a custom-designed physics-informed loss function which guides its training. The loss function consists of two parts. The first part is the loss contributed from the differential equation which is given as</p> \[\operatorname{Loss}_{\mathrm{GE}}=\operatorname{MSE}\left(\frac{d \hat{\theta}}{d t}-\widehat{\omega}, 0\right)+\operatorname{MSE}\left(\frac{d \widehat{\omega}}{d t}+b \cdot \widehat{\omega}+\left(\frac{g}{L}\right) \cdot \sin (\hat{\theta}), 0\right)\] <p>where \(\frac{d \hat{\theta}}{d t}\) and \(\frac{d \widehat{\omega}}{d t}\) are the gradients of the predicted \(\theta\) and \(\omega\) and are obtained via automatic differentiation. The second part of the loss function measures the loss contributed from the initial conditions of the IVP, namely \(\theta(0)\) and \(\omega\), and is given as</p> \[\operatorname{Loss}_{\mathrm{IC}}=\operatorname{MSE}\left(\hat{\theta}_0, \theta_0\right)+\operatorname{MSE}\left(\widehat{\omega}_0, \omega_0\right)\] <p>Then, the total loss is then the sum of the two parts and is given as</p> \[\operatorname{Loss}_{\mathrm{TOTAL}}=\operatorname{Loss}_{\mathrm{GE}}+\operatorname{Loss}_{\mathrm{IC}}\] <p>Below is a diagram of the architecture of the PINN.</p> <div class="row mt-3 l-page"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/NN_Final_Project/PINN_architecture_transparent.png" sizes="95vw"/> <img src="/assets/img/NN_Final_Project/PINN_architecture_transparent.png" class="img-fluid rounded" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> PINN Architecture. Diagram credit: <a href="https://github.com/parvanovkp/Neural_Networks_Final_Project/blob/main/PINN_architecture_transparent.png">Kal Parvanov/GitHub</a> </div> <p>Here is a specific Python implementation of this architecture with <code class="language-plaintext highlighter-rouge">tensorflow</code>. Please refer to the <a href="#appendix">Appendix</a> for a link to the entire code.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="c1"># Neural Network Model
</span><span class="k">class</span> <span class="nc">DampedPendulumModel</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layers_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">tanh</span><span class="sh">'</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">output_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">layers_list</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="nf">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">output_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Physics-Informed Loss Function
</span><span class="k">def</span> <span class="nf">compute_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">t_data</span><span class="p">,</span> <span class="n">y0</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nc">GradientTape</span><span class="p">(</span><span class="n">persistent</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="n">tape</span><span class="p">.</span><span class="nf">watch</span><span class="p">(</span><span class="n">t_data</span><span class="p">)</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">t_data</span><span class="p">)</span>
        <span class="n">theta</span><span class="p">,</span> <span class="n">omega</span> <span class="o">=</span> <span class="n">predictions</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">],</span> <span class="n">predictions</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span>

    <span class="c1"># Compute gradients outside the context of the tape
</span>    <span class="n">dtheta_dt</span> <span class="o">=</span> <span class="n">tape</span><span class="p">.</span><span class="nf">gradient</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">t_data</span><span class="p">,</span> <span class="n">unconnected_gradients</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">UnconnectedGradients</span><span class="p">.</span><span class="n">ZERO</span><span class="p">)</span>
    <span class="n">domega_dt</span> <span class="o">=</span> <span class="n">tape</span><span class="p">.</span><span class="nf">gradient</span><span class="p">(</span><span class="n">omega</span><span class="p">,</span> <span class="n">t_data</span><span class="p">,</span> <span class="n">unconnected_gradients</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">UnconnectedGradients</span><span class="p">.</span><span class="n">ZERO</span><span class="p">)</span>
    <span class="k">del</span> <span class="n">tape</span>  <span class="c1"># Delete the persistent tape after gradients are computed
</span>
    <span class="c1"># Damped pendulum equation
</span>    <span class="n">damped_eq</span> <span class="o">=</span> <span class="n">domega_dt</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">omega</span> <span class="o">+</span> <span class="p">(</span><span class="n">g</span> <span class="o">/</span> <span class="n">L</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>

    <span class="c1"># Loss for ODE and initial conditions
</span>    <span class="n">ode_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">square</span><span class="p">(</span><span class="n">dtheta_dt</span> <span class="o">-</span> <span class="n">omega</span><span class="p">)</span> <span class="o">+</span> <span class="n">tf</span><span class="p">.</span><span class="nf">square</span><span class="p">(</span><span class="n">damped_eq</span><span class="p">))</span>
    <span class="n">ic_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">square</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">y0</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">ode_loss</span> <span class="o">+</span> <span class="n">ic_loss</span>

<span class="c1"># Training Function
</span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">t_data</span><span class="p">,</span> <span class="n">y0</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">t_eval</span><span class="p">):</span>
    <span class="n">loss_history</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nc">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="nf">compute_loss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">t_data</span><span class="p">,</span> <span class="n">y0</span><span class="p">)</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="p">.</span><span class="nf">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">trainable_variables</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">apply_gradients</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">trainable_variables</span><span class="p">))</span>
        <span class="n">loss_history</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">loss</span><span class="p">.</span><span class="nf">numpy</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">loss_history</span>
</code></pre></div></div> <p>Note that, just like it was for the VNN, the physics informed loss function is incorporated into the model within the <code class="language-plaintext highlighter-rouge">train</code> function, where the <code class="language-plaintext highlighter-rouge">GradientTape</code> method is used for the loss function’s integration within the TensorFlow back-propagation framework.</p> <hr/> <h2 id="results-model-outcomes-and-insights">Results: Model Outcomes and Insights</h2> <p>This section covers the results of the two models and concludes with a comparison. The first objective is to train the VNN model on \(\theta\) and \(\omega\) data from the time interval \([0,10]\) and demonstrate its ability to accurately approximate the true solutions \(\theta(t)\) and \(\omega(t)\) on that interval. Note that since Neural Networks are by nature universal approximators, training the VNN on the data from the interval \([0,10]\) will allow it to learn the behavior of the dynamical system entirely.<d-cite key="HORNIK1991251"></d-cite> This is why the concept of using test data to evaluate the performance of the model, especially within the same time interval the neural network was trained on, is not applicable. The second goal is to train the PINN model on the interval \([0,20]\) and demonstrate its ability to solve the IVP on the interval \([0,10]\). Please note that since the PINN model does not use any \(\theta\) and \(\omega\) data during its training, training it on the interval \([0,20]\) still allows for a fair comparison to the VNN model. Finally, in the comparison section the two models are compared via their ability to predict the solution of the IVP on the interval \([0,20]\) in order to demonstrate the superior architecture of the PINN model and its suitability for dynamical systems problems such as the damped unforced pendulum problem.</p> <h3 id="vnn">VNN</h3> <p>The Vanilla Neural Network model was trained for 30001 epochs on 250 equispaced IVP solution points<d-footnote>For both $\theta$ and $\omega$.</d-footnote> on the interval \([0,10]\), obtained through the numerical solver. Below is a dynamic visualization of the training process that demonstrates neural networks’ ability to accurately approximate \(\theta(t)\) and \(\omega(t)\) in sufficiently many epochs.<d-footnote>Note that due to the nature of the model's reliance on training data there is some overfitting, which can be observed in the gif visualization below as a flickering deviations in the predicted solution throughout the epochs.</d-footnote></p> <div class="row mt-3 l-page"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/NN_Final_Project/training_progress_VNN.gif" sizes="95vw"/> <img src="/assets/img/NN_Final_Project/training_progress_VNN.gif" class="img-fluid rounded" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Visualization of the VNN learning process. GIF credit: <a href="https://github.com/parvanovkp/Neural_Networks_Final_Project/blob/main/Pendulum%20Project/Vanilla_NN_Pendulum_Project/training_progress_VNN.gif">Kal Parvanov/GitHub</a> </div> <p>The final training epoch resulted in a model that closely approximates the true solution of the IVP on the interval \([0,10]\).</p> <div class="row mt-3 l-page"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/NN_Final_Project/vnn_plot_epoch_30001.png" sizes="95vw"/> <img src="/assets/img/NN_Final_Project/vnn_plot_epoch_30001.png" class="img-fluid rounded" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Trained VNN model vs the true solution. Image credit: <a href="https://github.com/parvanovkp/Neural_Networks_Final_Project/blob/main/Pendulum%20Project/Vanilla_NN_Pendulum_Project/plot_epoch_30001.png">Kal Parvanov/GitHub</a> </div> <p>The associated training loss throughout the epochs can be seen in the following log base 10 loss plot.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/NN_Final_Project/Vanilla_Model_Loss_History.png" sizes="95vw"/> <img src="/assets/img/NN_Final_Project/Vanilla_Model_Loss_History.png" class="img-fluid rounded" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Log 10 plot of the training loss throughout the 30001 epochs. Image credit: <a href="https://github.com/parvanovkp/Neural_Networks_Final_Project/blob/main/Pendulum%20Project/Vanilla_NN_Pendulum_Project/Vanilla_Model_Loss_History.png">Kal Parvanov/GitHub</a> </div> <h3 id="pinn">PINN</h3> <p>The Physics-Informed Neural Network model was trained for 30001 epochs on 500 points from the interval \([0,20]\). Below is a dynamic visalization of the training process that demonstrates neural networks’ ability to accurately approximate \(\theta(t)\) and \(\omega(t)\) in sufficiently many epochs.<d-footnote>Note that due to the model's sole reliance on its loss function and the longer time interval for training the process of learning the true solution $\theta(t), \omega(t)$ on the interval $[0,20]$ is slower than that for the VNN model.</d-footnote></p> <div class="row mt-3 l-page"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/NN_Final_Project/training_progress.gif" sizes="95vw"/> <img src="/assets/img/NN_Final_Project/training_progress.gif" class="img-fluid rounded" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Visualization of the PINN learning process. GIF credit: <a href="https://github.com/parvanovkp/Neural_Networks_Final_Project/blob/main/Pendulum%20Project/PINN_Pendulum_Project/training_progress.gif">Kal Parvanov/GitHub</a> </div> <p>The final training epoch resulted in a model that produces a good approximation of the actual solution, which, however, is not sufficiently good for engineering purposes. For a better approximation of the quality of the VNN approximation more training epochs are needed.</p> <div class="row mt-3 l-page"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/NN_Final_Project/pinn_prediction_30001.png" sizes="95vw"/> <img src="/assets/img/NN_Final_Project/pinn_prediction_30001.png" class="img-fluid rounded" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Trained PINN model vs the true solution. Image credit: <a href="https://github.com/parvanovkp/Neural_Networks_Final_Project/blob/main/Pendulum%20Project/PINN_Pendulum_Project/training_plots/prediction_30001.png">Kal Parvanov/GitHub</a> </div> <p>The associated training loss throughout the epochs can be seen in the following log base 10 loss plot.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/NN_Final_Project/model_loss_history_pinn.png" sizes="95vw"/> <img src="/assets/img/NN_Final_Project/model_loss_history_pinn.png" class="img-fluid rounded" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Log 10 plot of the training loss throughout the 30001 epochs. Image credit: <a href="https://github.com/parvanovkp/Neural_Networks_Final_Project/blob/main/Pendulum%20Project/PINN_Pendulum_Project/model_loss_history.png">Kal Parvanov/GitHub</a> </div> <p>As we can see from the Log 10 plot of the loss throughout the epochs for the PINN model, there is less overfitting as evidenced by the lower jumps in loss at higher epochs. This suggests that the PINN model is a more stable approach for solving the IVP. The reliance on a physics-informed loss function helps mitigate the overfitting issues arising from the underlying structure of ANNs.</p> <h3 id="comparison-of-results">Comparison of Results</h3> <p>The VNN and PINN models are two very different approaches to solving the the pendulum IVP. While the Vanilla Neural Network model takes a traditional data-dependent deep learning approach, the Physics-Informed Neural Network uses an entirely novel approach that combines some of the best aspects of the worlds of numerical solvers and deep learning. The non-reliance on training data of the PINN allows it to focus on more accurately learning the underlying dynamics of the problem through its physics-informed loss function. This results in a model that not only requires no-data to train, but also has the ability to almost instantenously predict on the entire interval of training without the need for interpolation. To demonstrate this visually the VNN and PINN models were used to predict the IVP solution on the interval \([0,20]\).</p> <div class="row mt-3 l-screen"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/NN_Final_Project/prediction_outside_training.png" sizes="95vw"/> <img src="/assets/img/NN_Final_Project/prediction_outside_training.png" class="img-fluid rounded" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> VNN prediction on the interval [0,20]. Image credit: <a href="https://github.com/parvanovkp/Neural_Networks_Final_Project/blob/main/Pendulum%20Project/Vanilla_NN_Pendulum_Project/prediction_outside_training.png">Kal Parvanov/GitHub</a> </div> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/NN_Final_Project/pinn_prediction_outside_training.png" sizes="95vw"/> <img src="/assets/img/NN_Final_Project/pinn_prediction_outside_training.png" class="img-fluid rounded" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> PINN prediction on the interval [0,20]. Image credit: <a href="https://github.com/parvanovkp/Neural_Networks_Final_Project/blob/main/Pendulum%20Project/PINN_Pendulum_Project/pinn_prediction_outside_training.png">Kal Parvanov/GitHub</a> </div> </div> </div> <p>As can be seen from the plots above the Vanilla Neural Network accurately approximates the true solution on its interval of training, but it fails to predict the solution outside of the interval \([0,10]\). This, of course, is to be expected and is an excellent example of typical Neural Networks reliance on training data for making predictions. This over-reliance on data could often mislead or downright prevent engineers and researchers from gaining valuable insights about the problem, especially if the training data is tainted for some reason. The Physics-Informed Neural Network on the other hand demonstrates its ability to learn the IVP solution outside the original interval of interest, without the need to use any training data. This opens the door for a lot of applications of PINNs for solving differential equations that were previously thought unachievable by Neural Networks. With sufficient training one can use a PINN to solve complex differential equations almost instantaneously and to a great degree of accuracy. Below is a dynamic visualization of the training process of a PINN over 150001 epochs on the pendulum IVP.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/NN_Final_Project/training_progress_DDE_PINN.gif" sizes="95vw"/> <img src="/assets/img/NN_Final_Project/training_progress_DDE_PINN.gif" class="img-fluid rounded" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Visualization of the PINN learning process over 150001 epochs. GIF credit: <a href="https://github.com/parvanovkp/Neural_Networks_Final_Project/blob/main/Pendulum%20Project/DDE_PINN/training_progress_DDE_PINN.gif">Kal Parvanov/GitHub</a> </div> <h3 id="neural-networks-as-interpolators">Neural Networks as Interpolators</h3> <p>An interesting aspect of both the PINN and VNN models, which is also shared by all ANNs<d-footnote>Artificial Neural Networks.</d-footnote>, is their ability to approximate functions.<d-cite key="HORNIK1991251"></d-cite> The log 10 error plots for the difference between the true (numerical solver) and predicted theta for the VNN and PINN tell a very interesting story.<d-footnote>Note that the same phenomenon can be seen in the log 10 error plots for $\omega$ which have been omitted for brevity.</d-footnote></p> <div class="row mt-3 l-page"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/NN_Final_Project/log_error_theta_vnn.png" sizes="95vw"/> <img src="/assets/img/NN_Final_Project/log_error_theta_vnn.png" class="img-fluid rounded" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Log 10 error plot for VNN on the interval [0,20]. Image credit: <a href="https://github.com/parvanovkp/Neural_Networks_Final_Project/blob/main/Pendulum%20Project/Vanilla_NN_Pendulum_Project/log_error_theta.png">Kal Parvanov/GitHub</a> </div> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/NN_Final_Project/log_error_theta.png" sizes="95vw"/> <img src="/assets/img/NN_Final_Project/log_error_theta.png" class="img-fluid rounded" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Log 10 error plot for PINN on the interval [0,20]. Image credit: <a href="https://github.com/parvanovkp/Neural_Networks_Final_Project/blob/main/Pendulum%20Project/PINN_Pendulum_Project/log_error_theta.png">Kal Parvanov/GitHub</a> </div> </div> </div> <p>The log error plots above exhibit an interesting arching pattern<d-footnote> On the interval $[0,10]$ for the VNN and on the interval $[0,20]$ for the PINN</d-footnote>, which a person with an introductory numerical analysis background might recognize as <a href="https://en.wikipedia.org/wiki/Runge%27s_phenomenon">Runge’s phenomenon</a> which is a particular type of arch-like oscillation that occurs when using polynomial interpolation with polynomials of high degree over a set of equispaced interpolation points. From here follows that we can improve our results for both models by using time values (\(t\)) that are selected as the zeros of the \(n\)-th <a href="https://en.wikipedia.org/wiki/Chebyshev_nodes">Chebyshev polynomial</a>. This a classical approach in the field of Numerical Analysis and has a solid theoretical background that has established its usefulness.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/NN_Final_Project/2560px-Chebyshev_Zeros.svg.png" sizes="95vw"/> <img src="/assets/img/NN_Final_Project/2560px-Chebyshev_Zeros.svg.png" class="img-fluid rounded" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A plot of the zeros of the first 50 Chebyshev polynomials of the first kind. Image credit: <a href="https://commons.wikimedia.org/wiki/File:Chebyshev_Zeros.svg">Glosser.ca/Wikipedia</a> </div> <p>Finally, the observance of the Runge phenomenon suggests that Neural Networks can be thought of as interpolators instead of as approximators as the training loss approaches 0. Here is a <a href="https://gowrishankar.info/blog/deep-learning-is-not-as-impressive-as-you-think-its-mere-interpolation/">link</a> to the summary of a very interesting and thought provoking discussion between Prof. Yann LeCun and colleagues on Twitter from 2021.</p> <hr/> <h2 id="conclusion-broader-implications-and-future-research-directions">Conclusion: Broader Implications and Future Research Directions</h2> <p>The pendulum problem, once a simple concept in classical mechanics, has evolved into a complex subject integral to contemporary engineering and science. Its journey began in the 17th century with Christiaan Huygens using it for timekeeping. This historical application laid the groundwork for further scientific breakthroughs, including Earth’s rotational proof and Galileo’s discoveries. In modern times, the pendulum transcends its initial purpose, playing a crucial role in seismology and structural engineering. Its principles, particularly in vibration control, are vital in designing earthquake-resistant structures. This evolution from a rudimentary timekeeping device to a key component in sophisticated systems highlights the pendulum’s significance. It exemplifies how basic mechanical concepts can be applied to solve complex, real-world problems. The story of the pendulum is a testament to the adaptability and enduring relevance of fundamental scientific principles.</p> <p>Advancements in dynamical systems theory have led to a deeper understanding of the pendulum in realistic scenarios. Traditional models, previously simplistic, now incorporate external forces like air resistance and friction. These more complex models reveal intricate behaviors, including transitions to chaotic motions under certain conditions. This richer understanding has highlighted the limitations of conventional analytical methods, especially in predicting dynamic behaviors accurately. The need for advanced analytical approaches has become increasingly apparent. These approaches must capture the nuanced dynamics of systems like the pendulum. The challenge lies in accurately modeling systems that exhibit both regular and irregular behaviors. This advancement in understanding dynamical systems marks a significant evolution in scientific analysis and modeling.</p> <div class="row mt-3 l-gutter"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/NN_Final_Project/Double_pendulum_predicting_dynamics.gif" sizes="95vw"/> <img src="/assets/img/NN_Final_Project/Double_pendulum_predicting_dynamics.gif" class="img-fluid rounded" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption l-gutter" style="margin-top: -60px;"> A chaotic double pendulum dynamical system. GIF credit: <a href="https://commons.wikimedia.org/wiki/File:Double_pendulum_predicting_dynamics.gif">Jacopo Bertolotti/Wikipedia</a> </div> <p>The emergence of numerical analysis has addressed the limitations of traditional methods in handling complex systems. In fields where precision and computational efficiency are paramount, such as aerospace engineering and high-rise building design, these methods are essential. The development of these techniques has been driven by the need for faster, more accurate solutions. These new methods aim to provide deeper insights into complex systems at reduced computational costs. The shift towards more efficient modeling techniques represents a significant change in computational analysis. It signals a new era in understanding and predicting the behavior of dynamic systems. The growing complexity of these systems necessitates a continual evolution of analytical methods. This evolution is crucial for keeping pace with the increasing sophistication of engineering and scientific applications.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/NN_Final_Project/Chaos_Theory_&amp;_Double_Pendulum_-_3.jpg" sizes="95vw"/> <img src="/assets/img/NN_Final_Project/Chaos_Theory_&amp;_Double_Pendulum_-_3.jpg" class="img-fluid rounded" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The butterfly effect in a chaotic double pendulum dynamical system. Image credit: <a href="https://commons.wikimedia.org/wiki/File:Chaos_Theory_%26_Double_Pendulum_-_3.jpg">Christian V./Wikipedia</a> </div> <p>The introduction of Neural Networks, especially the Physics-Informed Neural Network (PINN), has revolutionized the field. The Vanilla Neural Network (VNN) serves as a foundational model, leveraging function approximation capabilities. In contrast, the PINN introduces an innovative approach by integrating physics principles directly into the network. This allows the PINN to efficiently solve complex differential equations without extensive data reliance. These networks, particularly the PINN, have redefined predictive modeling in dynamics. They have opened new avenues for understanding and analyzing dynamic systems. The unique features of these networks demonstrate a significant advancement in the field. They highlight the potential for new, data-independent approaches to modeling dynamic systems with neural networks.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <img src="https://upload.wikimedia.org/wikipedia/commons/3/3e/Lorenz_attractor.gif" class="img-fluid rounded" alt="A streamline in a Lorenz system"/> </div> </div> <div class="caption"> "A streamline, seen as a flow of particles from the same initial point, in a Lorenz system." GIF credit: <a href="https://commons.wikimedia.org/wiki/File:Lorenz_attractor.gif">Thierry Dugnolle/Wikipedia</a> </div> <p>Looking forward, Operator Learning and methods like DeepONet represent the next frontier in dynamical system analysis. Building on the foundation laid by PINNs, these techniques hold great promise for solving complex differential equations.<d-cite key="lu2021"></d-cite> They aim to capture the underlying operators governing system dynamics, offering more precise and adaptable solutions. This represents not just a technical advancement but a paradigm shift in how dynamic systems are approached. The potential applications of these methods are vast and varied. They could lead to breakthroughs in fields ranging from environmental science to financial modeling. The integration of these advanced techniques will further enhance our ability to understand and predict complex physical phenomena. This progression signifies a major leap in scientific problem-solving and modeling capabilities.</p> <h2 id="appendix">Appendix</h2> <p><strong>Image Credits:</strong> Thumbnail GIF by <a href="https://commons.wikimedia.org/wiki/File:Anchor_escapement_animation_217x328px.gif">Chetvorno/Wikipedia</a></p> <p><strong>Project Code:</strong> <a href="https://github.com/parvanovkp/Neural_Networks_Final_Project/tree/main/Pendulum%20Project">Kal Parvanov/Github</a></p>]]></content><author><name>Kaloyan Parvanov</name></author><category term="neural networks"/><category term="neural networks"/><category term="differential equations"/><summary type="html"><![CDATA[Applying physics-informed neural networks to ODEs]]></summary></entry></feed>