<!DOCTYPE html> <html lang="en, bg, de"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="3m9Oob5fw7DxBVE7aIi63DaBCHjNFPeluUH9jCmmMBA"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Button Game: When Should You Stop for the Best Prize? | Kaloyan P. Parvanov </title> <meta name="author" content="Kaloyan P. Parvanov"> <meta name="description" content="An interesting interview question about optimal stopping."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.css" integrity="sha256-uRX+PiRTR4ysKFRCykT8HLuRCub26LgXJZym3Yeom1c=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/Favicon2.png?7d25c01b23eb8ef730d6b6ad8c347aaa"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://www.kparvanov.com/blog/2025/button_game/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Kaloyan</span> P. Parvanov </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Button Game: When Should You Stop for the Best Prize?</h1> <p class="post-meta"> Created in June 24, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/optimal-stopping"> <i class="fa-solid fa-hashtag fa-sm"></i> optimal stopping</a>   <a href="/blog/tag/backward-induction"> <i class="fa-solid fa-hashtag fa-sm"></i> backward induction</a>   <a href="/blog/tag/dynamic-programming"> <i class="fa-solid fa-hashtag fa-sm"></i> dynamic programming</a>   <a href="/blog/tag/probability"> <i class="fa-solid fa-hashtag fa-sm"></i> probability</a>   ·   <a href="/blog/category/mathematics"> <i class="fa-solid fa-tag fa-sm"></i> mathematics</a>   <a href="/blog/category/computer-science"> <i class="fa-solid fa-tag fa-sm"></i> computer science</a>   <a href="/blog/category/data-analysis"> <i class="fa-solid fa-tag fa-sm"></i> data analysis</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="an-interview-puzzle">An Interview Puzzle</h2> <p>While preparing for an upcoming technical interview, I came across a probability puzzle that, at first glance, seems like a simple game of chance, but quickly reveals layers of strategic depth. The problem was as follows:</p> <blockquote> <p>An agent can press a button up to 10 times. Each press generates a random monetary value, drawn from a continuous uniform distribution on the interval <code class="language-plaintext highlighter-rouge">[0, $100,000]</code>. After each press, the agent can either stop and keep the current value, ending the game, or discard the value and continue to the next press. If the agent completes all 10 presses, they must accept the value from the 10th press.</p> </blockquote> <p><strong>The question is: What is the optimal strategy to maximize the expected payoff?</strong></p> <p>My initial intuition was to work backward from the end. But after solving it, I became more interested in the bigger picture: what kind of problem is this, and what mathematical principles guarantee that the solution is truly optimal?</p> <h2 id="zooming-out-the-world-of-optimal-stopping">Zooming Out: The World of Optimal Stopping</h2> <p>This puzzle is not a one-off brain teaser; it’s a classic example of a <strong>finite-horizon optimal stopping problem</strong>. This class of problems is fundamental in fields like finance, economics, and computer science, dealing with the challenge of choosing the perfect time to take a particular action to maximize a reward or minimize a cost.</p> <p>To analyze such problems rigorously, we model them as <a href="https://en.wikipedia.org/wiki/Markov_decision_process" rel="external nofollow noopener" target="_blank"><strong>Markov Decision Processes (MDPs)</strong></a>. An MDP is a mathematical framework for decision-making in situations where outcomes are partly random and partly under the control of a decision-maker. Our game fits this framework perfectly:</p> <ol> <li> <strong>States:</strong> The state at any point is simply the number of presses remaining, \(n\).</li> <li> <strong>Actions:</strong> In any state (where \(n &gt; 1\)), our actions are to <code class="language-plaintext highlighter-rouge">stop</code> or <code class="language-plaintext highlighter-rouge">continue</code>.</li> <li> <strong>The Markov Property:</strong> Most importantly, the game is “memoryless.” The decision we make with \(n\) presses left depends only on the current state and the number we just drew, not on the history of previously rejected values. Each press is an independent event.</li> </ol> <p>Recognizing the problem as an MDP is the first step, as it allows us to bring a powerful toolkit to bear on finding the solution: the theory of dynamic programming.</p> <h2 id="dynamic-programming-and-bellmans-principle">Dynamic Programming and Bellman’s Principle</h2> <p>The cornerstone for solving MDPs is <strong>dynamic programming</strong>, a method for breaking down a complex problem into a sequence of simpler, nested subproblems. The philosophical foundation of this method is <a href="https://en.wikipedia.org/wiki/Bellman_equation#Bellman's_principle_of_optimality" rel="external nofollow noopener" target="_blank"><strong>Bellman’s Principle of Optimality</strong></a>.</p> <blockquote> <p><strong>Bellman’s Principle of Optimality</strong> states that an optimal policy has the property that whatever the initial state and first decision are, the remaining decisions must constitute an optimal policy for the subproblem starting from the new state.</p> </blockquote> <p>In simpler terms, a perfect overall plan is composed of a series of perfect sub-plans. To play the 10-press game optimally, our strategy for the remaining 9 presses (should we continue) must also be the optimal strategy for a 9-press game.</p> <p>This principle is captured mathematically in the <a href="https://en.wikipedia.org/wiki/Bellman_equation" rel="external nofollow noopener" target="_blank"><strong>Bellman Equation</strong></a>. For our type of optimal stopping problem, the equation takes the following form. If we let \(V_n\) be the maximum expected payoff we can get with \(n\) presses remaining, then:</p> \[V_n = \mathbb{E}[\max(X_n, V_{n-1})]\] <p>This elegant equation is the blueprint for our strategy. It says the value of being in a state with \(n\) presses left is the expected value of the <em>best possible choice</em>: either the random value \(X_n\) we just drew, or the value \(V_{n-1}\) we expect to get if we discard the current draw and play optimally from the next state.</p> <h2 id="applying-the-theory-to-solve-the-puzzle">Applying the Theory to Solve the Puzzle</h2> <p>With this theoretical framework, we can now solve the interview puzzle methodically using <strong>backward induction</strong>. For clarity, let’s set \(C = 100,000\).</p> <p><strong>Step 1: Solve the Base Case (\(n=1\))</strong> We start at the end. With only one press left, we must accept the outcome. The expected value is the mean of the uniform distribution: \(V_1 = \mathbb{E}[X] = \frac{C}{2} = \$50,000\)</p> <p><strong>Step 2: Derive the General Recurrence Relation</strong> For any stage \(n \ge 2\), the decision rule is to stop if the draw \(x\) is greater than the continuation value, \(V_{n-1}\). We can find the value \(V_n\) by calculating the expectation from the Bellman Equation:</p> \[V_n = \mathbb{E}[\max(X, V_{n-1})] = \int_{0}^{C} \max(x, V_{n-1}) \frac{dx}{C}\] <p>We split the integral at the threshold \(V_{n-1}\), since the value of \(\max(x, V_{n-1})\) changes at that point:</p> \[V_n = \frac{1}{C} \left[ \int_{0}^{V_{n-1}} V_{n-1} \,dx + \int_{V_{n-1}}^{C} x \,dx \right]\] <p>Now, we solve the integrals:</p> \[V_n = \frac{1}{C} \left[ V_{n-1} [x]_0^{V_{n-1}} + \left[\frac{x^2}{2}\right]_{V_{n-1}}^C \right]\] \[V_n = \frac{1}{C} \left[ V_{n-1}(V_{n-1} - 0) + \left(\frac{C^2}{2} - \frac{V_{n-1}^2}{2}\right) \right]\] \[V_n = \frac{1}{C} \left[ V_{n-1}^2 + \frac{C^2 - V_{n-1}^2}{2} \right] = \frac{2V_{n-1}^2 + C^2 - V_{n-1}^2}{2C}\] <p>This simplifies to the clean, general recurrence relation we can use for all steps:</p> \[V_n = \frac{V_{n-1}^2 + C^2}{2C}\] <p>Using this formula, we can now iterate backward from our known base case (\(V_1 = 50,000\)) to generate the complete optimal strategy. For instance:</p> <ul> <li> <strong>For n=2:</strong> \(V_2 = \frac{(50000)^2 + (100000)^2}{2 \cdot 100000} = \$62,500\)</li> <li> <strong>For n=3:</strong> \(V_3 = \frac{(62500)^2 + (100000)^2}{2 \cdot 100000} \approx \$69,531\)</li> </ul> <p>Repeating this process for all 10 stages gives us the following policy:</p> <table> <thead> <tr> <th style="text-align: left">Presses Left (n)</th> <th style="text-align: left">Threshold to Stop (\(V_{n-1}\))</th> <th style="text-align: left">Expected Payoff (\(V_n\))</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">10</td> <td style="text-align: left">$84,982</td> <td style="text-align: left">$86,110</td> </tr> <tr> <td style="text-align: left">9</td> <td style="text-align: left">$83,645</td> <td style="text-align: left">$84,982</td> </tr> <tr> <td style="text-align: left">8</td> <td style="text-align: left">$82,030</td> <td style="text-align: left">$83,645</td> </tr> <tr> <td style="text-align: left">7</td> <td style="text-align: left">$80,038</td> <td style="text-align: left">$82,030</td> </tr> <tr> <td style="text-align: left">6</td> <td style="text-align: left">$77,508</td> <td style="text-align: left">$80,038</td> </tr> <tr> <td style="text-align: left">5</td> <td style="text-align: left">$74,173</td> <td style="text-align: left">$77,508</td> </tr> <tr> <td style="text-align: left">4</td> <td style="text-align: left">$69,531</td> <td style="text-align: left">$74,173</td> </tr> <tr> <td style="text-align: left">3</td> <td style="text-align: left">$62,500</td> <td style="text-align: left">$69,531</td> </tr> <tr> <td style="text-align: left">2</td> <td style="text-align: left">$50,000</td> <td style="text-align: left">$62,500</td> </tr> <tr> <td style="text-align: left">1</td> <td style="text-align: left">(Must accept draw)</td> <td style="text-align: left">$50,000</td> </tr> </tbody> </table> <p><em>Note: Dollar values are rounded to the nearest dollar.</em></p> <h2 id="empirical-validation-with-python-simulation">Empirical Validation with Python Simulation</h2> <p>As a final check, a Monte Carlo simulation confirms that this strategy works in practice.</p> <h3 id="implementation">Implementation</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">random</span>

<span class="k">def</span> <span class="nf">calculate_thresholds</span><span class="p">(</span><span class="n">max_presses</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mi">100000</span><span class="p">):</span>
    <span class="n">V</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">V</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">C</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">max_presses</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">V_n_minus_1</span> <span class="o">=</span> <span class="n">V</span><span class="p">[</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">V</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">V_n_minus_1</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">C</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">C</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">V</span>

<span class="k">def</span> <span class="nf">play_one_game</span><span class="p">(</span><span class="n">thresholds</span><span class="p">,</span> <span class="n">max_presses</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mi">100000</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">presses_left</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_presses</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">current_draw</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">presses_left</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">current_draw</span>
        <span class="n">threshold</span> <span class="o">=</span> <span class="n">thresholds</span><span class="p">[</span><span class="n">presses_left</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">current_draw</span> <span class="o">&gt;=</span> <span class="n">threshold</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">current_draw</span>
    <span class="k">return</span> <span class="mi">0</span> 

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="n">MAX_PRESSES</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">C</span> <span class="o">=</span> <span class="mi">100000</span>
    <span class="n">NUM_SIMULATIONS</span> <span class="o">=</span> <span class="mi">1_000_000</span>
    <span class="n">thresholds</span> <span class="o">=</span> <span class="nf">calculate_thresholds</span><span class="p">(</span><span class="n">MAX_PRESSES</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Theoretical Expected Payoff: $</span><span class="si">{</span><span class="n">thresholds</span><span class="p">[</span><span class="n">MAX_PRESSES</span><span class="p">]</span><span class="si">:</span><span class="p">,.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">total_payoff</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="nf">play_one_game</span><span class="p">(</span><span class="n">thresholds</span><span class="p">,</span> <span class="n">MAX_PRESSES</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">NUM_SIMULATIONS</span><span class="p">))</span>
    <span class="n">average_payoff</span> <span class="o">=</span> <span class="n">total_payoff</span> <span class="o">/</span> <span class="n">NUM_SIMULATIONS</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Simulated Average Payoff (</span><span class="si">{</span><span class="n">NUM_SIMULATIONS</span><span class="si">:</span><span class="p">,</span><span class="si">}</span><span class="s"> games): $</span><span class="si">{</span><span class="n">average_payoff</span><span class="si">:</span><span class="p">,.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="nf">main</span><span class="p">()</span>
</code></pre></div></div> <h3 id="results">Results</h3> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Theoretical Expected Payoff: $86,109.82

Simulated Average Payoff (1,000,000 games): $86,112.80
</code></pre></div></div> <p>The simulation beautifully converges to the theoretical result.</p> <h2 id="why-the-strategy-is-optimal">Why the Strategy is Optimal</h2> <p>This brings us to the decisive question: why is this threshold rule not merely good, but provably <strong>optimal</strong>? The answer rests on two foundational results.</p> <p>Because the game is a finite-horizon Markov decision process, it terminates after ten presses. For such problems:</p> <ul> <li> <a href="https://en.wikipedia.org/wiki/Bellman_equation#Bellman's_principle_of_optimality" rel="external nofollow noopener" target="_blank"><strong>Bellman’s Principle of Optimality</strong></a> guarantees that an overall best plan must embed a best plan for every remaining sub-game.</li> <li>The corresponding <a href="https://en.wikipedia.org/wiki/Bellman_equation" rel="external nofollow noopener" target="_blank"><strong>Bellman Equation</strong></a>: \(V_n = \mathbb{E}\!\bigl[\max\!\bigl(X_n,\;V_{n-1}\bigr)\bigr],\)</li> </ul> <p>solved backward from \(V_1 = C/2\), delivers the unique fixed point of the dynamic-programming operator.</p> <p>The fixed point yields the concrete rule <strong>“stop when the draw \(X \ge V_{n-1}\).”</strong> Because the Bellman operator is a contraction, no alternative policy, no matter how elaborate, can exceed the expected payoff \(V_{10}\). Each comparison of the current draw \(x\) with the continuation value \(V_{n-1}\) therefore constitutes the globally optimal decision at that step (technically, the threshold rule is the first hitting time of the <a href="https://en.wikipedia.org/wiki/Snell_envelope" rel="external nofollow noopener" target="_blank"><em>Snell envelope</em></a>, guaranteeing no other stopping rule can beat it).</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Kaloyan P. Parvanov. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: June 30, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.js" integrity="sha256-4rppopQE9POKfukn2kEvhJ9Um25Cf6+IDVkARD0xh78=" crossorigin="anonymous"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-D9ELQTS46J"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-D9ELQTS46J");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"This is a collection of most of the projects I have worked on so far in my career.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-the-st-petersburg-paradox-re-run-over-time",title:"The St. Petersburg Paradox, Re-Run Over Time",description:"How an old paradox reveals a deep flaw in classical economics and points the way to a more realistic model of human decision-making.",section:"Posts",handler:()=>{window.location.href="/blog/2025/st_petersburg/"}},{id:"post-the-button-game-when-should-you-stop-for-the-best-prize",title:"The Button Game: When Should You Stop for the Best Prize?",description:"An interesting interview question about optimal stopping.",section:"Posts",handler:()=>{window.location.href="/blog/2025/button_game/"}},{id:"post-detecting-regime-shifts-in-sp500-stocks-using-pca-and-sparse-pca",title:"Detecting Regime Shifts in SP500 Stocks Using PCA and Sparse PCA",description:"This project explores PCA and Sparse PCA on 457 SP500 stocks, using 2-minute interval data over 31 trading days (August 8 to September 19, 2024). The focus is on experimenting with dimensionality reduction techniques to identify regime shifts and key factors driving stock returns.",section:"Posts",handler:()=>{window.location.href="/blog/2024/distill/"}},{id:"post-the-damped-unforced-pendulum-problem",title:"The Damped Unforced Pendulum Problem",description:"Applying physics-informed neural networks to ODEs",section:"Posts",handler:()=>{window.location.href="/blog/2023/distill/"}},{id:"projects-physics-informed-neural-networks",title:"Physics-Informed Neural Networks",description:"Solving the unforced damped pendulum problem",section:"Projects",handler:()=>{window.location.href="/projects/PINN_project/"}},{id:"projects-regimeshift",title:"RegimeShift",description:"PCA Analysis of the SP500 Stocks",section:"Projects",handler:()=>{window.location.href="/projects/RegimeShift/"}},{id:"projects-eagle-eye",title:"Eagle Eye",description:"AI-Powered Search Engine",section:"Projects",handler:()=>{window.location.href="/projects/eagle_eye_project/"}},{id:"projects-gmail-smart-labeler",title:"GMail Smart Labeler",description:"AI-Powered Email Labeler",section:"Projects",handler:()=>{window.location.href="/projects/gmail_smart_labeler/"}},{id:"projects-mathbuddy",title:"MathBuddy",description:"AI-Powered Math Tutor",section:"Projects",handler:()=>{window.location.href="/projects/mathbuddy_project/"}},{id:"projects-tic-tac-toe",title:"Tic Tac Toe",description:"Tic-Tac-Toe AI with Alpha-Beta Pruning Minimax Algorithm",section:"Projects",handler:()=>{window.location.href="/projects/tic_tac_toe_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%70%61%72%76%61%6E%6F%76%6B%61%6C%6F%79%61%6E@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/parvanovkp","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/kparvanov","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/kparvanov1","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>